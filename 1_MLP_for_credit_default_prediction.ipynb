{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - MLP for credit default prediction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hotbread213/createClass/blob/master/1_MLP_for_credit_default_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEZwPezwngWV",
        "colab_type": "text"
      },
      "source": [
        "# IVADO Workshop Day 3 - Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PayaVFSVngWW",
        "colab_type": "text"
      },
      "source": [
        "This example will serve as a first hands-on experience with neural networks training using PyTorch. In this notebook, we will learn how to train a basic Multilayered Perceptron (MLP) and how to find good hyperparameters with the Hyperopt library. To achieve this, we will be experimenting with a credit card default dataset which was obtained from the UCI machine learning repository:\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
        "\n",
        "No need to download it from the original repository! We have it on our google drive and we will get it from there in the next cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UX2y82wngWW",
        "colab_type": "text"
      },
      "source": [
        "Let's begin by installing the packages that we will need today."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UoRcP3LpngWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install hyperopt\n",
        "!pip3 install googledrivedownloader\n",
        "!pip3 install tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnMivWiDngWc",
        "colab_type": "text"
      },
      "source": [
        "And of course importing them..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dci0L9pngWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To download our dataset\n",
        "from google_drive_downloader import GoogleDriveDownloader\n",
        "\n",
        "# Visualization\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Preprocessing and scoring\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, log_loss, precision_recall_curve\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.fixes import signature\n",
        "\n",
        "# Hyperparameter tuning\n",
        "from hyperopt import space_eval\n",
        "from hyperopt import fmin, hp, tpe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmH2MS3fngWe",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can download the credit default dataset using the unique Google Drive ID attached to it and save it directly in our current path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq5Yx4HtngWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CREDIT_ID = '1c0EKXaupL9Y_ttxKm0ujhnZ3QSYXNO9f'\n",
        "\n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id=CREDIT_ID,\n",
        "                                                      dest_path='./credit_data',\n",
        "                                                      unzip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yst2zz_hngWg",
        "colab_type": "text"
      },
      "source": [
        "Great, we now that we have all the tools wee need. Let's take a look at our data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwAVCvacngWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "credit_df = pd.read_csv('./UCI_Credit_Card.csv').drop('ID', axis=1).drop_duplicates()\n",
        "\n",
        "print('Nb. examples, Nb. features : {}'.format(credit_df.shape))\n",
        "print('Proportion of default: {}'.format(credit_df.iloc[:,-1].mean()))\n",
        "\n",
        "credit_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAS9gVfEngWj",
        "colab_type": "text"
      },
      "source": [
        "Each line represents attributes of a credit card owner from a Taiwanese bank. We have 24 columns in total, from which the last one is a binary target indicating if a client is going to default on his credit card payment in the next month. Below is a detailed description of the column features:\n",
        "\n",
        "- LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n",
        "- SEX: Gender (1=male, 2=female)\n",
        "- EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
        "- MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
        "- AGE: Age in years\n",
        "- PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
        "- PAY_2: Repayment status in August, 2005 (scale same as above)\n",
        "- PAY_3: Repayment status in July, 2005 (scale same as above)\n",
        "- PAY_4: Repayment status in June, 2005 (scale same as above)\n",
        "- PAY_5: Repayment status in May, 2005 (scale same as above)\n",
        "- PAY_6: Repayment status in April, 2005 (scale same as above)\n",
        "- BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
        "- BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
        "- BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
        "- BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
        "- BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
        "- BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
        "- PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
        "- PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
        "- PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
        "- PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
        "- PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
        "- PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
        "- default.payment.next.month: Default payment (1=yes, 0=no)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBlzQokangWk",
        "colab_type": "text"
      },
      "source": [
        "We observe that many of the variables are categorical. Thus, their numerical values do not mean much to a neural network. We will have to identify and transform each of these categorical variables to a onehot vector, i.e a vector with zeros everywhere except in one position. This process will add columns to our inital dataset.\n",
        "\n",
        "For example, the education variable will be transformed as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC1RkzeTngWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "credit_df[['EDUCATION']].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orSwboWKngWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example = pd.get_dummies(credit_df, columns=['EDUCATION'])\n",
        "example[[col for col in example.columns if 'EDUCATION' in col]].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn-QLWQqngWp",
        "colab_type": "text"
      },
      "source": [
        "We identify our categorical, numerical and target variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b7V1XUbngWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CATEGORICALS = ['SEX',\n",
        "               'EDUCATION',\n",
        "               'MARRIAGE',\n",
        "               'PAY_0',\n",
        "               'PAY_2',\n",
        "               'PAY_3',\n",
        "               'PAY_3',\n",
        "               'PAY_4',\n",
        "               'PAY_5',\n",
        "               'PAY_6'] \n",
        "\n",
        "TARGET = 'default.payment.next.month'\n",
        "\n",
        "NUMERICALS = list(set(credit_df.columns) - set(CATEGORICALS + [TARGET]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpz2qJDPngWr",
        "colab_type": "text"
      },
      "source": [
        "Convert all categorical features to onehot representation and separate features from target:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YFPkr4ZngWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "credit_df = credit_df.astype(np.float64)\n",
        "credit_df = pd.get_dummies(credit_df, columns=CATEGORICALS)\n",
        "\n",
        "features = credit_df.drop(TARGET, axis=1)\n",
        "labels = credit_df[[TARGET]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smrQB-9_ngWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heENix29ngWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Mean of numerical features \\n')\n",
        "print(features[NUMERICALS].mean())\n",
        "print()\n",
        "print('Std. dev. of numerical features \\n')\n",
        "print(features[NUMERICALS].std())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss7lWUnGngWy",
        "colab_type": "text"
      },
      "source": [
        "Neural networks training is a very unstable process. Success highly depends on good weight initialization and the form of feature space we are working in. Especially, NN perform well when the inputs we feed them are between [-1,1]. It also helps to have the same variance across feature dimensions.\n",
        "\n",
        "Concerning our categorical features, their value can either be 0 or 1. This is perfect for a neural network. However our numerical features have different scales and means. For this reason, we will substract to each numerical column its mean and divide it by its standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbuGnyxIngWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.mode.chained_assignment = None  \n",
        "\n",
        "scaler = StandardScaler()\n",
        "features[NUMERICALS] = scaler.fit_transform(features[NUMERICALS])\n",
        "\n",
        "print('Mean of numerical features \\n')\n",
        "print(features[NUMERICALS].mean())\n",
        "print()\n",
        "print('Std. dev. of numerical features \\n')\n",
        "print(features[NUMERICALS].std())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I68WbDlgngW1",
        "colab_type": "text"
      },
      "source": [
        "We can now divide our dataset into three subsets. \n",
        "\n",
        "- Train: To fit our neural network's parameters.\n",
        "- Valid: To estimate good training hyperparameters from independant data.\n",
        "- Test: To evaluate the final performance of our network\n",
        "\n",
        "We will keep a 60-20-20 proportion for the split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Y2iKpeOFngW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(features,\n",
        "                                                      labels,\n",
        "                                                      shuffle=True,\n",
        "                                                      test_size=0.4)\n",
        "\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_valid,\n",
        "                                                    y_valid,\n",
        "                                                    shuffle=True,\n",
        "                                                    test_size=0.5)\n",
        "\n",
        "X_train.shape, X_valid.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBAY5hutngW5",
        "colab_type": "text"
      },
      "source": [
        "We have just arrived to the fun part. We are going to design our network's architecture and train it on our well prepared data.\n",
        "\n",
        "PyTorch uses a special class called Module to let users define their neural networks. This class has to be inherited by your custom network class and you must override its forward method. This method's name indicates that this is where the predictions are done given a tensor input (feed-forward). It will receive and return a torch tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEjqgQfhngW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_shape: int, n_hidden: int, n_output: int):\n",
        "        \n",
        "        # Initialize Module parent class\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        # Fully-connected layer\n",
        "        self.fc1 = nn.Linear(input_shape, n_hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "        # Add as many layers as you want here!\n",
        "        # We use a batch normalization layer to regularize training\n",
        "        self.bn = nn.BatchNorm1d(n_hidden)\n",
        "        \n",
        "        # Final output layer\n",
        "        self.fc2 = nn.Linear(n_hidden, n_output)\n",
        "        \n",
        "        \n",
        "    def forward(self, x) -> torch.tensor: \n",
        "        \n",
        "        # Forward pass\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.bn(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return torch.sigmoid(x) # Sigmoid output for binary classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wndk1KDQngW8",
        "colab_type": "text"
      },
      "source": [
        "Below we can set our experiments hyperparameters. We will need:\n",
        "\n",
        "- A number of hidden units for our network\n",
        "- Epochs: a number of times that we are going to loop through our full training dataset\n",
        "- Batch size: the number of example that we will use in a network's forward pass and loss estimation.\n",
        "- Learning rate: step-size to use by our optimizer. Bigger learning rate -> bigger gradient update at each batch.\n",
        "- An L2 regularization constant: A penality on the L2 norm of the network's weights to help it generalize.\n",
        "\n",
        "We could definitely use other hyperparmeters such as number of hidden layers, batch-normalization or not, dropout rate... but we will keep this example concise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb6MtUJTngW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_INPUTS = X_train.shape[1]\n",
        "HIDDEN_UNITS = 64\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.00001\n",
        "L2_REG = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRv3VSyKngW_",
        "colab_type": "text"
      },
      "source": [
        "Here we have our training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw-r6f11ngXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Instantiate our MLP with our problem's dimensions\n",
        "net = MLP(N_INPUTS, HIDDEN_UNITS, 1)\n",
        "\n",
        "# We need an optimizer for our network params, Adam is a popular and good choice.\n",
        "optimizer = Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=L2_REG)\n",
        "\n",
        "# And a loss function, BCE (for binary crossentropy) is used for binary classification, \n",
        "# where we have a sigmoid function on top of our network's output.\n",
        "criterion = nn.BCELoss()\n",
        "        \n",
        "# Two dicts of lists to keep track of loss and auroc during training\n",
        "historical_aurocs = {'train':[], 'valid':[]} \n",
        "historical_loss = {'train':[], 'valid':[]} \n",
        "\n",
        "# run the main training loop for number of epochs\n",
        "for epoch in tqdm(range(EPOCHS), desc='Epochs'):\n",
        "        \n",
        "    for i in range(len(X_train)//BATCH_SIZE + 1):\n",
        "        \n",
        "            # Begin and end of current batch in our DataFrame\n",
        "            start = i*BATCH_SIZE\n",
        "            end = min((i+1)*BATCH_SIZE,len(X_train))\n",
        "            \n",
        "            # We have to convert ou subset of examples to a torch tensor\n",
        "            X = torch.tensor(X_train[start:end].values.astype(np.float32))\n",
        "            y = torch.tensor(y_train[start:end].values.astype(np.float32))\n",
        "            \n",
        "            # Re-init torch's gradient or hence they will accumulate\n",
        "            optimizer.zero_grad()\n",
        "            # Do our predidction on considered batch\n",
        "            net_out = net(X)\n",
        "            # Compute loss\n",
        "            loss = criterion(net_out, y)\n",
        "            \n",
        "            # Do the gradient update\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "    # Use the no_grad context to evaluate the network,\n",
        "    # all the forward passes done will not be considered\n",
        "    # during the next gradient update. This is important.\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        X = torch.tensor(X_valid.values.astype(np.float32))\n",
        "        y = torch.tensor(y_valid.values.astype(np.float32))\n",
        "        net_out = net(X).detach().numpy()\n",
        "        valid_auroc = roc_auc_score(y.detach().numpy(), net_out)\n",
        "        valid_loss = log_loss(y.detach().numpy(), net_out)\n",
        "        \n",
        "        X = torch.tensor(X_train.values.astype(np.float32))\n",
        "        y = torch.tensor(y_train.values.astype(np.float32))\n",
        "        net_out = net(X).detach().numpy()\n",
        "        train_auroc = roc_auc_score(y.detach().numpy(), net_out)\n",
        "        train_loss = log_loss(y.detach().numpy(), net_out)\n",
        "        \n",
        "        historical_aurocs['valid'].append(valid_auroc)\n",
        "        historical_aurocs['train'].append(train_auroc)\n",
        "        \n",
        "        historical_loss['valid'].append(valid_loss)\n",
        "        historical_loss['train'].append(train_loss)\n",
        "        \n",
        "        \n",
        "    # Shuffle our training data to avoid repeating the same updates over and over.\n",
        "    X_train, y_train = shuffle(X_train, y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvh0ZFhZngXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(historical_aurocs['valid'], label='valid')\n",
        "plt.plot(historical_aurocs['train'], label='train')\n",
        "plt.title('AUROC Evolution during training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUROC')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h9CSs9nngXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(historical_loss['valid'], label='valid')\n",
        "plt.plot(historical_loss['train'], label='train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIlATxyXngXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_pr_curve(precision, recall, color, name, auroc, loss):\n",
        "    \n",
        "    step_kwargs = ({'step': 'post'}\n",
        "               if 'step' in signature(plt.fill_between).parameters\n",
        "               else {})\n",
        "\n",
        "    plt.step(recall, precision, color=color, alpha=0.5,\n",
        "             where='post', label=name + ' - AUROC : {:f} - Log-loss : {:f}'.format(auroc, loss))\n",
        "    \n",
        "\n",
        "plt.figure(figsize=(13,9))\n",
        "\n",
        "for dataset in [(X_train, y_train, 'black', 'Train'),\n",
        "                (X_valid, y_valid, 'blue', 'Valid'),\n",
        "                (X_test, y_test, 'red', 'Test')]:\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "            X = torch.tensor(dataset[0].values.astype(np.float32))\n",
        "            y = torch.tensor(dataset[1].values.astype(np.float32))\n",
        "            net_out = net(X).detach().numpy()\n",
        "\n",
        "            precision, recall, _  = precision_recall_curve(y.detach().numpy(), net_out)\n",
        "            auroc = roc_auc_score(y.detach().numpy(), net_out)\n",
        "            loss = log_loss(y.detach().numpy(), net_out)\n",
        "\n",
        "    plot_pr_curve(precision, recall, dataset[2], dataset[3], auroc, loss)\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('Precision-Recall curves on our 3 datasets')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ6Mv8V1ngXJ",
        "colab_type": "text"
      },
      "source": [
        "## Hyperopt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0usmNuYngXK",
        "colab_type": "text"
      },
      "source": [
        "In the past training, we fixed our hyperparameters in advance. In fact, they were previously found by manual trial and error. Perhaps there is better way to find a good combination of hyperparams without trying every possibility.\n",
        "\n",
        "A package called Hyperopt can help us with that. It uses the Tree-structured Parzen Estimator (TPE) algorithm to explore the space more efficiently than randomly or exhaustively.\n",
        "\n",
        "To use it, we will need to wrap all of our previous code inside one function, that we will call train_loop. This function will have a dictionary of hyperparameters as input. It will also receive by default our train and valid dataset splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCys3ui8ngXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_loop(params,\n",
        "               X_train=X_train,\n",
        "               y_train=y_train,\n",
        "               X_test=X_test,\n",
        "               y_test=y_test):\n",
        "    \n",
        "    N_INPUTS = X_train.shape[1]\n",
        "    HIDDEN_UNITS = params['hidden_units']\n",
        "    EPOCHS = 50\n",
        "    BATCH_SIZE = params['batch_size']\n",
        "    LEARNING_RATE = params['learning_rate']\n",
        "    L2_REG = params['l2_reg']\n",
        "    \n",
        "    net = MLP(N_INPUTS, HIDDEN_UNITS, 1)\n",
        "\n",
        "    optimizer = Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=L2_REG, amsgrad=True)\n",
        "    criterion = nn.BCELoss()\n",
        "    \n",
        "    historical_aurocs = {'train':[], 'valid':[]} \n",
        "    historical_loss = {'train':[], 'valid':[]} \n",
        "    \n",
        "    # run the main training loop\n",
        "    for epoch in tqdm(range(EPOCHS), desc='Epochs'):\n",
        "\n",
        "        for i in range(len(X_train)//BATCH_SIZE + 1):\n",
        "\n",
        "                start = i*BATCH_SIZE\n",
        "                end = min((i+1)*BATCH_SIZE,len(X_train))\n",
        "\n",
        "                X = torch.tensor(X_train[start:end].values.astype(np.float32))\n",
        "                y = torch.tensor(y_train[start:end].values.astype(np.float32))\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                net_out = net(X)\n",
        "                loss = criterion(net_out, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            X = torch.tensor(X_valid.values.astype(np.float32))\n",
        "            y = torch.tensor(y_valid.values.astype(np.float32))\n",
        "            net_out = net(X).detach().numpy()\n",
        "            valid_auroc = roc_auc_score(y.detach().numpy(), net_out)\n",
        "            valid_loss = log_loss(y.detach().numpy(), net_out)\n",
        "\n",
        "            X = torch.tensor(X_train.values.astype(np.float32))\n",
        "            y = torch.tensor(y_train.values.astype(np.float32))\n",
        "            net_out = net(X).detach().numpy()\n",
        "            train_auroc = roc_auc_score(y.detach().numpy(), net_out)\n",
        "            train_loss = log_loss(y.detach().numpy(), net_out)\n",
        "\n",
        "            historical_aurocs['valid'].append(valid_auroc)\n",
        "            historical_aurocs['train'].append(train_auroc)\n",
        "\n",
        "            historical_loss['valid'].append(valid_loss)\n",
        "            historical_loss['train'].append(train_loss)\n",
        "        \n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "    \n",
        "    return min(historical_loss['valid'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob3hNX_cngXL",
        "colab_type": "text"
      },
      "source": [
        "We can define our hyperparameter search space with prior knowledge of what could be good hyperparameters. After that, we can feed our train loop and our hyperparameter space in Hyperopt's optimizing function fmin. We have to specify a number of trials to limit the number of combinations we are willing to try. For this example, we use only ten trials to find a good set of hyperparameters because our time is limited. It is obviously too low considering there are $4^{4}=256$ possible combinations in our space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSZV_8l4ngXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparam_space = {'l2_reg': hp.choice('l2_reg', [0.001,0.0001,0.00001,0.000001]),\n",
        "                   'learning_rate': hp.choice('learning_rate', [0.001,0.0001,0.00001,0.000001]),\n",
        "                   'hidden_units': hp.choice('hidden_units', [32, 64, 128, 256]),\n",
        "                   'batch_size': hp.choice('batch_size', [32, 64, 128, 256])}\n",
        "\n",
        "best = fmin(fn=train_loop,\n",
        "            space=hyperparam_space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQHdCDPxngXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Our \"optimal\" hyperparameters are ...')\n",
        "print(space_eval(hyperparam_space, best))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXKO5of4ngXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}