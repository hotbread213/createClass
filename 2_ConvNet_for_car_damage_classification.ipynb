{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 - ConvNet for car damage classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hotbread213/createClass/blob/master/2_ConvNet_for_car_damage_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAUYAPtxoKOm",
        "colab_type": "text"
      },
      "source": [
        "# IVADO Workshop Day 3 - Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG_YpU6zoKOn",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial, our goal will be to classify different car damages. We have scraped more than 1600 images of damaged cars from google. Those were labeled according to the severity of their damage. We end up with two classes in our dataset, one for major damages (labeled as 1) and another for minor damages (labeled as 0).\n",
        "\n",
        "To accomplish our goal to discrimate between the two, we will be using a convolutional network that we are going to implement in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "XYLOtmN_oKOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install googledrivedownloader\n",
        "!pip3 install tqdm\n",
        "!pip3 install seaborn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0kjWPdFoKOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data downlooad\n",
        "from google_drive_downloader import GoogleDriveDownloader\n",
        "\n",
        "# Deep learning and manipulation\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "\n",
        "#Evaluation\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import roc_auc_score, log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Visualization\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRJQJtUQoKOu",
        "colab_type": "text"
      },
      "source": [
        "In this example, because of the high computational cost of our task, we will be using a GPU. To let PyTorch know that we use a GPU instead of a regular CPU, we will need to specify our device and mount all torch tensors on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca6z5DSxoKOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda:0' if torch.cuda.device_count()>0 else 'cpu:0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJXXG57SoKOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlpuikikoKO0",
        "colab_type": "text"
      },
      "source": [
        "Let's download and load our data in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm_Xk7HJoKO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CARS_ID = '1uI64vTdE-1geqmUq2azU_cwqtdnQMkrI'\n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id=CARS_ID,\n",
        "                                                      dest_path='./car_damages_1600',\n",
        "                                                      unzip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGNsk23IoKO2",
        "colab_type": "text"
      },
      "source": [
        "We will keep only the first 850 examples of each class to have balanced classes for our experiment. This will make our predictions more interpretable in terms of accuracy (if we choose a probability threshold of 0.5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFWJlosHoKO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N=850\n",
        "major = np.load('./major_damages.npy')[:N]\n",
        "minor = np.load('./minor_damages.npy')[:N]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SesM1ckcoKO4",
        "colab_type": "text"
      },
      "source": [
        "We will create our own labels, merge the two classes and shuffle everything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q3nK8F6oKO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = np.vstack([major, minor])\n",
        "labels = np.vstack([np.ones(N)[:, None], np.zeros(N)[:,None]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1WWjdmnoKO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images.shape, labels.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEvB6w_UoKO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, labels = shuffle(images, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dEk4tMToKO_",
        "colab_type": "text"
      },
      "source": [
        "The following function can help us visualize our images. The images were previously resized to 256x256 pixels. It is easier to train a convnet when the images have a square size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ota0Fhr1oKPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_4_random(images, title=''):\n",
        "    \n",
        "    idx = np.random.randint(len(images),size=(2,2))\n",
        "    \n",
        "    fig, axes = plt.subplots(2,2)\n",
        "\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            axes[i][j].imshow(images[idx[i,j]])\n",
        "            axes[i][j].get_xaxis().set_visible(False)\n",
        "            axes[i][j].get_yaxis().set_visible(False)\n",
        "            axes[i][j].set_frame_on(False)\n",
        "    \n",
        "    fig.set_size_inches((14,14))\n",
        "    fig.suptitle(title, y=0.95)\n",
        "    \n",
        "    return idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15wUyMOzoKPB",
        "colab_type": "text"
      },
      "source": [
        "Let's check what our data looks like! If you find any weird image, two images that look very similar or one that seems to belong to the wrong class, please let us know. It is a new dataset and google image is not always right so we will clean it for future experiments!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FFH7CLvoKPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_4_random(images[labels.flatten()==1], title='Major car damages')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlK6Kfj5oKPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_4_random(images[labels.flatten()==0], title='Minor car damages')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIGTa4kMoKPH",
        "colab_type": "text"
      },
      "source": [
        "Let's split our data set in train, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e21G0oruoKPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(images, labels, test_size=0.4)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhewscngoKPJ",
        "colab_type": "text"
      },
      "source": [
        "... and we are ready do define our 2D convolutional network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdVDFS_YoKPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  \n",
        "    def __init__(self, hidden_units):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3,8,3)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(3)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(8,8,3)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(3)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(8,16,3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(3)\n",
        "        \n",
        "        self.conv4 = nn.Conv2d(16,32,3)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.pool4 = nn.MaxPool2d(3)\n",
        "        \n",
        "        self.fc1 = nn.Linear(32*2*2, hidden_units)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_units, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.pool3(self.relu3(self.conv3(x)))\n",
        "        x = self.pool4(self.relu4(self.conv4(x)))\n",
        "        \n",
        "        #We have to reshape the output of the convolutional+pooling layers\n",
        "        #before feeding it into the dense ones.\n",
        "        x = self.fc1(x.view(-1,32*2*2))\n",
        "        x = self.relu5(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return torch.sigmoid(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk0cEXe8oKPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CNN(64).forward(torch.ones((1,3,256,256))).size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RT2KtV4oKPP",
        "colab_type": "text"
      },
      "source": [
        "We here define our hyperparameters, feel free to tweak them and experiment with different combinations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwTsRKVCoKPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_UNITS = 64\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3P2LTrxoKPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Instantiate our CNN with a choice of hidden units\n",
        "net = CNN(HIDDEN_UNITS).to(DEVICE)\n",
        "\n",
        "# We need an optimizer for our network params, Adam is a popular and good choice.\n",
        "optimizer = Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# And a loss function, BCE (for binary crossentropy) is used for binary classification, \n",
        "# where we have a sigmoid function on top of our network's output.\n",
        "criterion = nn.BCELoss().to(DEVICE)\n",
        "        \n",
        "# Two dicts of lists to keep track of loss and auroc during training\n",
        "historical_acc = {'train':[], 'valid':[]} \n",
        "historical_loss = {'train':[], 'valid':[]} \n",
        "\n",
        "# run the main training loop for number of epochs\n",
        "for epoch in tqdm(range(EPOCHS), desc='Epochs'):\n",
        "        \n",
        "    for i in tqdm(range(len(X_train)//BATCH_SIZE + 1), desc='Batch'):\n",
        "        \n",
        "            # Begin and end of current batch in our array\n",
        "            start = i*BATCH_SIZE\n",
        "            end = min((i+1)*BATCH_SIZE,len(X_train))\n",
        "            \n",
        "            # We have to convert ou subset of examples to a torch tensor\n",
        "            X = torch.tensor(X_train[start:end].astype(np.float32)).permute(0,3,1,2).to(DEVICE)\n",
        "            y = torch.tensor(y_train[start:end].astype(np.float32)).to(DEVICE)\n",
        "            \n",
        "            # Re-init torch's gradient or hence they will accumulate\n",
        "            optimizer.zero_grad()\n",
        "            # Do our predidction on considered batch\n",
        "            net_out = net(X)\n",
        "            # Compute loss\n",
        "            loss = criterion(net_out, y)\n",
        "            \n",
        "            # Do the gradient update\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "    # Use the no_grad context to evaluate the network,\n",
        "    # all the forward passes done will not be considered\n",
        "    # during the next gradient update. This is important.\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        X = torch.tensor(X_valid.astype(np.float32)).permute(0,3,1,2).to(DEVICE)\n",
        "        y = torch.tensor(y_valid.astype(np.float32)).to(DEVICE)\n",
        "        net_out = net(X).detach().cpu().numpy()\n",
        "        valid_acc = (y.detach().cpu().numpy()==(net_out>0.5)).mean()\n",
        "        valid_loss = log_loss(y.detach().cpu().numpy(), net_out)\n",
        "        \n",
        "        X = torch.tensor(X_train.astype(np.float32)).permute(0,3,1,2).to(DEVICE)\n",
        "        y = torch.tensor(y_train.astype(np.float32)).to(DEVICE)\n",
        "        net_out = net(X).detach().cpu().numpy()\n",
        "        train_acc = (y.detach().cpu().numpy()==(net_out>0.5)).mean()\n",
        "        train_loss = log_loss(y.detach().cpu().numpy(), net_out)\n",
        "        \n",
        "        historical_acc['valid'].append(valid_acc)\n",
        "        historical_acc['train'].append(train_acc)\n",
        "        \n",
        "        historical_loss['valid'].append(valid_loss)\n",
        "        historical_loss['train'].append(train_loss)\n",
        "        \n",
        "    # Shuffle our training data to avoid repeating the same updates over and over.\n",
        "    X_train, y_train = shuffle(X_train, y_train)\n",
        "    \n",
        "    print('Epoch {} --- Train acc: {:f} - Valid acc. {:f} --- Train loss: {:f} - Valid loss: {:f}'.format(epoch,\n",
        "                                                                                                 train_acc,\n",
        "                                                                                                 valid_acc,\n",
        "                                                                                                 train_loss,\n",
        "                                                                                                 valid_loss))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzyFDgvLoKPW",
        "colab_type": "text"
      },
      "source": [
        "At the opposite of our previous example, where our two classes where pretty imbalanced, we can now evaluate our model accroding to the prediction accuracy. Let's visualize how the training went."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwlAC2l7oKPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(1,2)\n",
        "\n",
        "axes[0].plot(historical_loss['valid'], label='Valid')\n",
        "axes[0].plot(historical_loss['train'], label='Train')\n",
        "axes[0].set_xlabel('Epochs')\n",
        "axes[0].set_ylabel('Log-loss')\n",
        "\n",
        "axes[1].plot(historical_acc['valid'])\n",
        "axes[1].plot(historical_acc['train'])\n",
        "axes[1].set_xlabel('Epochs')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "\n",
        "fig.suptitle('Evolution of metrics during training')\n",
        "fig.set_size_inches((16,8))\n",
        "fig.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThlQwQ9aoKPZ",
        "colab_type": "text"
      },
      "source": [
        "We can inspect how our convnet managed to classify the examples of both class with a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptQnCPuqoKPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "        \n",
        "        X = torch.tensor(X_test.astype(np.float32)).permute(0,3,1,2).to(DEVICE)\n",
        "        y = torch.tensor(y_test.astype(np.float32)).to(DEVICE)\n",
        "        net_out = net(X).detach().cpu().numpy()\n",
        "        test_acc = (y.detach().cpu().numpy()==(net_out>0.5)).mean()\n",
        "        test_loss = log_loss(y.detach().cpu().numpy(), net_out)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "conf_mat = confusion_matrix(y.detach().cpu().numpy(), net_out>0.5)\n",
        "sns.heatmap(conf_mat, annot=True, cmap='Greys', fmt='.1f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCXJY4OvoKPc",
        "colab_type": "text"
      },
      "source": [
        "Let's visualize some of of the bad classification to see if there was any ambiguity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vTapBSDoKPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=y.detach().cpu().numpy() \n",
        "false_positives = X_test[(y.flatten()!=(net_out>0.5).flatten()) & (y==0).flatten()]\n",
        "false_negatives = X_test[(y.flatten()!=(net_out>0.5).flatten()) & (y==1).flatten()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "eb6RPpZRoKPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_4_random(false_positives, 'False positives')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59ZVcBdhoKPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_4_random(false_negatives, 'False negatives')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciOsnAW8oKPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}