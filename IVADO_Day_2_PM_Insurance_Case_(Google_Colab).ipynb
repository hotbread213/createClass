{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IVADO - Day 2 PM - Insurance_Case (Google Colab).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "_ZtY5EhcMGa6",
        "dLXRqVPaMGa6",
        "Ni7v_PmRMGa7",
        "zGoGc72FMGa8"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hotbread213/createClass/blob/master/IVADO_Day_2_PM_Insurance_Case_(Google_Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e196NkMBMGZP",
        "colab_type": "text"
      },
      "source": [
        "## Insurance Case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySCvaU4kMGZR",
        "colab_type": "text"
      },
      "source": [
        "### Part 1) Study of the insurance database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1QRb7dyMGZR",
        "colab_type": "text"
      },
      "source": [
        "Libraries importation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih4l6B83MGZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import r2_score \n",
        "from functools import reduce\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google_drive_downloader import GoogleDriveDownloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vno6SojZMGZV",
        "colab_type": "text"
      },
      "source": [
        "Download the datasets needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoIa3DQwMGZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Constants:\n",
        "    \n",
        "    SEED = 1\n",
        "    \n",
        "    # Paths to the data files\n",
        "    Cluster_variables = '/Cluster_variables.csv' \n",
        "    Data_category = '/Data_category.csv' \n",
        "    INSEE_CODES = '/INSEE_CODES.csv' \n",
        "    PG_2017_CLAIMS_YEAR0 = '/PG_2017_CLAIMS_YEAR0.csv' \n",
        "    PG_2017_YEAR0 = '/PG_2017_YEAR0.csv' \n",
        "    \n",
        "    # Google drive id to be able to download from drive\n",
        "    Cluster_variables_ID = '1duFLPcauNQSyRoWGafw0E_W8MQnu-S7z'\n",
        "    Data_category_ID = '1y96sh5rOexFuWVGerDqtOhaMH8hQRUqe'\n",
        "    INSEE_CODES_ID = '1-M8ah4fKmRSrICaR7dBETDJNk7b1AB9k'\n",
        "    PG_2017_CLAIMS_YEAR0_ID = '1CJAUfq-624qXYxMaahi7XcV16OMWDoWQ'\n",
        "    PG_2017_YEAR0_ID = '1JaLa2adWDEhapFJ4W-3kG7jHB7psP5B1'\n",
        "    \n",
        "constants = Constants\n",
        "random.seed(constants.SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSdLMjU2MGZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /Cluster_variables.csv\n",
        "!rm /Data_category.csv\n",
        "!rm /INSEE_CODES.csv\n",
        "!rm /PG_2017_CLAIMS_YEAR0.csv\n",
        "!rm /PG_2017_YEAR0.csv\n",
        "\n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id=constants.Cluster_variables_ID, dest_path=constants.Cluster_variables, unzip=False)\n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id=constants.Data_category_ID, dest_path=constants.Data_category, unzip=False)\n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id=constants.INSEE_CODES_ID, dest_path=constants.INSEE_CODES, unzip=False)\n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id=constants.PG_2017_CLAIMS_YEAR0_ID, dest_path=constants.PG_2017_CLAIMS_YEAR0, unzip=False)\n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id=constants.PG_2017_YEAR0_ID, dest_path=constants.PG_2017_YEAR0, unzip=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct2JHx2iMGZZ",
        "colab_type": "text"
      },
      "source": [
        "Dataset importation:\n",
        "- PG_2017_Year0 is the underwriting dataset with 100,000 insured at time 0. It contains 31 features for each policy.\n",
        "- Thus, it's a matrix of size 100000 X 31."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxK5DO6EMGZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "insured_data = pd.read_csv(constants.PG_2017_YEAR0, header=0)\n",
        "insured_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i672p-UdMGZc",
        "colab_type": "text"
      },
      "source": [
        "Importation of the claim dataset.\n",
        "- Every row of this file contains a specific claim associated with a policy. \n",
        "- The number of rows in 'PG_2017_CLAIMS_YEAR0.csv' is smaller than 'PG_2017_YEAR0.csv', since the majority of the policies got zero claim during 2017. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ParysYbMGZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "claims_data = pd.read_csv(constants.PG_2017_CLAIMS_YEAR0, header=0)\n",
        "claims_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY5xfOp7MGZf",
        "colab_type": "text"
      },
      "source": [
        "Note that the claims data doesn't have the \"id_policy\" needed to join the two dataframes. We create it by concatenation the \"id_client\" and \"id_vehicle\" columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCl2BZ6RMGZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "claims_data[\"id_policy\"] = claims_data[\"id_client\"] + \"-\" + claims_data[\"id_vehicle\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euKFFADaMGZh",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the same \"id_policy\" in both dataframes, we can merge them in order to have all the claims made for\n",
        "each policy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1GzzkRpMGZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merged_data = pd.merge(insured_data, claims_data[[\"id_policy\", \"claim_nb\", \"claim_amount\"]], on=\"id_policy\", how=\"left\")\n",
        "merged_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPI3tDOjMGZl",
        "colab_type": "text"
      },
      "source": [
        "We can now compute the total number of claims, and the total claims amount per policy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXCBeBNgMGZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_claims_number = merged_data.groupby(\"id_policy\", as_index=False)[[\"claim_nb\"]].sum()\n",
        "total_claims_amount = merged_data.groupby(\"id_policy\", as_index=False)[[\"claim_amount\"]].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pg3sh61MGZo",
        "colab_type": "text"
      },
      "source": [
        "We now have the complete dataset on which we will perform statistical analysis after cleaning it, of course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CJj1caWMGZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete_data = reduce(lambda left, right: pd.merge(left, right, on=\"id_policy\", how=\"left\"),\n",
        "                       [insured_data, total_claims_number, total_claims_amount])\n",
        "complete_data[['claim_nb', 'claim_amount']] = complete_data[['claim_nb', 'claim_amount']].fillna(0)\n",
        "complete_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYLIM6i1MGZq",
        "colab_type": "text"
      },
      "source": [
        "It's always good practice to explore the dataset before training any ML algorithms on it.\n",
        "This is what will briefly do here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFo14begMGZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete_data.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGBJqusJMGZt",
        "colab_type": "text"
      },
      "source": [
        "What can we say with this information?\n",
        "\n",
        "1) The data seems very clean!\n",
        "\n",
        "2) There seems to be a missing value in \"vh_age\". We replace the missing value with the same vh_model mean. This is called mean substitution, and it is one of the most popular imputation techniques (not necessarily the best one).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiWhvWF7MGZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete_data[\"vh_age\"] = complete_data.groupby(\"vh_model\")[\"vh_age\"].apply(lambda x: x.fillna(x.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxJ3EC7mMGZw",
        "colab_type": "text"
      },
      "source": [
        "We compute basic statistics on our dataset.\n",
        "\n",
        "How many policies does the dataset contain?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxO5b3KaMGZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"Total number of policies: {0:2d}\".format(complete_data['id_policy'].nunique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoEYExBjMGZy",
        "colab_type": "text"
      },
      "source": [
        "How many clients?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQfxHovYMGZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"Total number of clients: {0:2d}\".format(complete_data['id_client'].nunique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0nu4EXUMGZ0",
        "colab_type": "text"
      },
      "source": [
        "Statistical description of the numerical columns of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8clJRwiBMGZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete_data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEktTNBKMGZ4",
        "colab_type": "text"
      },
      "source": [
        "Do you notice anything strange? What about drv_age_lic1 v.s. drv_age1 and drv_age_lic2 v.s. drv_age2?\n",
        "\n",
        "Does having vh_cyl values at 0 make sense? Same question with vh_value and vh_weight.\n",
        "\n",
        "What should we do with these rows (i.e. data points)? What could be the cause behind this anomaly? For the time being, we will simply\n",
        "delete these rows from our analysis. In practice we would like to understand why these errors are present, and correct\n",
        "them if possible (verification with other datasets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0P38JP5MGZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete_data = complete_data[complete_data[\"drv_age1\"] >= complete_data[\"drv_age_lic1\"]+17]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF83l5TGMGZ9",
        "colab_type": "text"
      },
      "source": [
        "Since certain policies have no secondary driver, we have to get rid of the zeros to study the columns that are related\n",
        "to the secondary drivers: drv_age2, drv_sex2, and drv_age_lic2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYzixkLkMGZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_drivers_data = complete_data.loc[insured_data[\"drv_drv2\"] == \"Yes\", [\"drv_age2\", \"drv_sex2\", \"drv_age_lic2\"]]\n",
        "second_drivers_data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZni2rO_MGaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"Number of inadequate second driver ages: {0:2d}\".format(len(second_drivers_data[second_drivers_data[\"drv_age2\"] <\n",
        "                                                                   second_drivers_data[\"drv_age_lic2\"]+16]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fytWnsSKMGaC",
        "colab_type": "text"
      },
      "source": [
        "What's happening here?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH3-PMP8MGaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_drivers_data[second_drivers_data[\"drv_age2\"] <\n",
        "                          second_drivers_data[\"drv_age_lic2\"]][\"drv_age_lic2\"].unique()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRQnm2_NMGaF",
        "colab_type": "text"
      },
      "source": [
        "Again, we are getting rid of this data to continue our analysis. We are talking about a somewhat good amount of data.\n",
        "In practice, we would have to understand the origin of the error. This isn't possible here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4BZRUUCMGaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete_data = complete_data[(complete_data[\"drv_age2\"] >= complete_data[\"drv_age_lic2\"]+16) |\n",
        "                              (complete_data[\"drv_drv2\"] == \"No\")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0WBemUJMGaH",
        "colab_type": "text"
      },
      "source": [
        "Let's replace the vh_value of 0 with the average value of the same vh_model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev_RGUfSMGaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete_data[\"vh_value\"] = complete_data.groupby(\"vh_model\")[\"vh_value\"].apply(lambda x: x.replace(0, x[x > 0].mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6Ro5-o_MGaJ",
        "colab_type": "text"
      },
      "source": [
        "We do the same thing with the weight:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnKXm5WTMGaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete_data[\"vh_weight\"] = complete_data.groupby(\"vh_model\")[\"vh_weight\"].apply(lambda x: x.replace(0, x[x > 0].mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd2G0ZnnMGaL",
        "colab_type": "text"
      },
      "source": [
        "Now that we know that the dataset is finally clean, we can start visualizing the data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeK9N7T-MGaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_data = complete_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaLYQ2fiMGaO",
        "colab_type": "text"
      },
      "source": [
        "We generate simple histograms for the categorical variables, and that gives us a good first overview of our insured\n",
        "portfolio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4E91AkHMGaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figure = plt.figure()\n",
        "plt.figure(figsize=(22,30))\n",
        "figure.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "# What's the first driver's sex distribution on these policies?\n",
        "plt.subplot(3, 3, 1)\n",
        "clean_data['drv_sex1'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Sex distribution of the auto insurance policies (first driver)\")\n",
        "plt.xlabel(\"Sex\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the secondary driver's sex distribution on these policies?\n",
        "plt.subplot(3, 3, 2)\n",
        "clean_data.loc[clean_data['drv_drv2'] == \"Yes\", \"drv_sex2\"].value_counts().plot(kind='bar')\n",
        "plt.title(\"Sex distribution of the auto insurance policies (secondary driver)\")\n",
        "plt.xlabel(\"Sex\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the coverage distribution of the insurance portfolio?\n",
        "plt.subplot(3, 3, 3)\n",
        "clean_data['pol_coverage'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Coverage policies distribution\")\n",
        "plt.xlabel(\"Type\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the mileage-based policy subscription proportion of the portfolio?\n",
        "plt.subplot(3, 3, 4)\n",
        "clean_data['pol_payd'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Mileage-based policy distribution\")\n",
        "plt.xlabel(\"Subscription\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the payment frequency distribution of the policies?\n",
        "plt.subplot(3, 3, 5)\n",
        "clean_data['pol_pay_freq'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Payment frequency distribution\")\n",
        "plt.xlabel(\"Payment Frequency\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the vehicle usage distribution?\n",
        "plt.subplot(3, 3, 6)\n",
        "clean_data['pol_usage'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Vehicle usage distribution\")\n",
        "plt.xlabel(\"Usage\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the motor alimentation distribution?\n",
        "plt.subplot(3, 3, 7)\n",
        "clean_data['vh_fuel'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Motor alimentation distribution\")\n",
        "plt.xlabel(\"Alimentation\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# How much policies have a second driver?\n",
        "plt.subplot(3, 3, 8)\n",
        "clean_data['drv_drv2'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Secondary drivers distribution\")\n",
        "plt.xlabel(\"Secondary driver\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the vehicle type distribution?\n",
        "plt.subplot(3, 3, 9)\n",
        "clean_data['vh_type'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Vehicle type distribution\")\n",
        "plt.xlabel(\"Vehicle type\")\n",
        "plt.ylabel(\"Policies\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMc52KogMGaQ",
        "colab_type": "text"
      },
      "source": [
        "For categorical variables with a large number of classes, histograms are not really appropriate to analyze the data.\n",
        "\n",
        "We create a pie chart describing the 5 most popular car makers in our portfolio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhNF_pPMMGaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[car_makers, car_numbers] = [clean_data[\"vh_make\"].value_counts().index.tolist(),\n",
        "                             clean_data[\"vh_make\"].value_counts().as_matrix()]\n",
        "others_number = car_numbers[5:].sum()\n",
        "car_makers, car_numbers = car_makers[:5], car_numbers[:5]\n",
        "car_makers.append(\"OTHERS\")\n",
        "car_numbers = np.append(car_numbers, others_number)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.pie(car_numbers, labels=car_makers, autopct='%1.1f%%')\n",
        "plt.title(\"Distribution of automakers\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzTAm_c6MGaS",
        "colab_type": "text"
      },
      "source": [
        "Let's analyze the geographical data.\n",
        "\n",
        "We will import the commune names from the table INSEE_CODES.csv that I created from the official INSEE table\n",
        "\"Table d'appartenance géographique des communes au 1er janvier 2011\" taken from\n",
        " https://www.insee.fr/fr/information/2028028. More information on each INSEE code can be extracted from this table,\n",
        "which are very useful for pricing (e.g. Tranche aire urbaine (TDUU2010), catégorie de commune (CATAEU2010) et\n",
        " population (POP_MUN_2009)). See the documentation for the exact meaning of these new variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DenVHJ9HMGaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "commune_data = pd.read_csv(constants.INSEE_CODES, header=0)\n",
        "commune_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrPbEmV-MGaV",
        "colab_type": "text"
      },
      "source": [
        "We join the commune data to the clean data by the INSEE code in order to have our final dataset that could be used for\n",
        "modeling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51_N60UCMGaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data = pd.merge(clean_data, commune_data, left_on=\"pol_insee_code\", right_on=\"code_geographique\", how=\"left\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbQaEU1PMGaY",
        "colab_type": "text"
      },
      "source": [
        "We create the department variable, which simply is the first two digits of the insee code, and we visualize them in a pie chart:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2yw_WRGMGaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data[\"department\"] = final_data[\"pol_insee_code\"].astype(str).str[:2]\n",
        "\n",
        "[department, department_numbers] = [final_data[\"department\"].value_counts().index.tolist(),\n",
        "                                    final_data[\"department\"].value_counts().as_matrix()]\n",
        "\n",
        "others_number = department_numbers[10:].sum()\n",
        "department, department_numbers = department[:10], department_numbers[:10]\n",
        "department.append(\"OTHERS\")\n",
        "department_numbers = np.append(department_numbers, others_number)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.pie(department_numbers, labels=department, autopct='%1.1f%%')\n",
        "plt.title(\"Departments distribution\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sOLza-FMGaa",
        "colab_type": "text"
      },
      "source": [
        "Surprisingly, a lot of policies are in the North of France (59th department). In second place, we have the 75th department,\n",
        "which is Paris. In third place, we have the 69th department, Rhône, which contains Lyon. The policies aren't condensed\n",
        "into one specific city/department, which is good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iczZ4UMEMGaa",
        "colab_type": "text"
      },
      "source": [
        "Now, let's analyze some continuous and discrete variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_8hBHABMGab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figure = plt.figure()\n",
        "plt.figure(figsize=(18,18))\n",
        "figure.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "# What's the bonus-malus coefficient distribution on these policies?\n",
        "plt.subplot(3, 3, 1)\n",
        "plt.hist(final_data['pol_bonus'])\n",
        "plt.title(\"Bonus-malus coefficient distribution\")\n",
        "plt.xlabel(\"Coefficient\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the policy duration distribution on these policies?\n",
        "plt.subplot(3, 3, 2)\n",
        "plt.hist(final_data['pol_duration'])\n",
        "plt.title(\"Policies duration distribution\")\n",
        "plt.xlabel(\"Duration\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the situation duration distribution of the insurance portfolio?\n",
        "plt.subplot(3, 3, 3)\n",
        "plt.hist(final_data['pol_sit_duration'])\n",
        "plt.title(\"Situation duration distribution\")\n",
        "plt.xlabel(\"Type\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the first driver's age distribution of the policies?\n",
        "plt.subplot(3, 3, 4)\n",
        "plt.hist(final_data['drv_age1'])\n",
        "plt.title(\"First driver's age distribution\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the second driver's age distribution of the policies?\n",
        "plt.subplot(3, 3, 5)\n",
        "plt.hist(final_data.loc[final_data[\"drv_drv2\"] == \"Yes\", 'drv_age2'])\n",
        "plt.title(\"Second driver's age distribution\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the first driver's licence age?\n",
        "plt.subplot(3, 3, 6)\n",
        "plt.hist(final_data['drv_age_lic1'])\n",
        "plt.title(\"First driver's licence age distribution\")\n",
        "plt.xlabel(\"Licence age\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the second driver's licence age?\n",
        "plt.subplot(3, 3, 7)\n",
        "plt.hist(final_data.loc[final_data['drv_drv2'] == 'Yes', 'drv_age_lic2'])\n",
        "plt.title(\"Second driver's licence age distribution\")\n",
        "plt.xlabel(\"Licence age\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# How old are the insured cars?\n",
        "plt.subplot(3, 3, 8)\n",
        "plt.hist(final_data['vh_sale_end'])\n",
        "plt.title(\"Years since the end of marketing years of the vehicle distribution\")\n",
        "plt.xlabel(\"Years\")\n",
        "plt.ylabel(\"Policies\")\n",
        "\n",
        "# What's the vehicle type distribution?\n",
        "plt.subplot(3, 3, 9)\n",
        "plt.hist(final_data['vh_value'])\n",
        "plt.title(\"Vehicle value distribution\")\n",
        "plt.xlabel(\"Value (euros)\")\n",
        "plt.ylabel(\"Policies\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2NgZ_lVMGad",
        "colab_type": "text"
      },
      "source": [
        "We will know analyze a little more the target variables, i.e. the number and amount of claims per policy.\n",
        "\n",
        "What are the claim frequencies?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlDoHKvHMGad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data[\"claim_nb\"].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ospKK9wIMGae",
        "colab_type": "text"
      },
      "source": [
        "It's also important to quantify the dependence relationship between our exogenous variables, and the endogenous\n",
        "variable. It will come in handy during the feature selection. We'll only do a couple, so that you can also do your own\n",
        "analysis.\n",
        "\n",
        "We start with the linear correlation coefficients for each continuous and discrete variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IESbIanRMGaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data.corr()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "829X56iYMGag",
        "colab_type": "text"
      },
      "source": [
        "The variables that have a high correlation coefficient with claim_nb or claim_amount might be the most\n",
        "important predictive variables in the future models. More advanced feature selection techniques need to be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4KgniG_MGah",
        "colab_type": "text"
      },
      "source": [
        "Linear relationship between vh_value and claim_amount:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Bjlga3MGah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regression = linear_model.LinearRegression()\n",
        "x = final_data.loc[(final_data[\"claim_amount\"] > 0) & (final_data[\"claim_amount\"] < 10000), \"vh_value\"].\\\n",
        "    as_matrix().reshape((-1,1))\n",
        "y = final_data.loc[(final_data[\"claim_amount\"] > 0) & (final_data[\"claim_amount\"] < 10000), \"claim_amount\"].\\\n",
        "    as_matrix().reshape((-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRf0OIesMGaj",
        "colab_type": "text"
      },
      "source": [
        "We fit the regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiMhfdPjMGak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regression.fit(x, y)\n",
        "prediction = regression.predict(x)\n",
        "\n",
        "plt.scatter(x, y, color='black')\n",
        "plt.plot(x, prediction, color='red')\n",
        "plt.xlabel(\"Vehicle value\")\n",
        "plt.ylabel(\"Claim amount\")\n",
        "plt.title(\"Claim amount regressed over vehicle age\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vJf2daqMGal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"Regression coefficient: \"+str(regression.coef_[0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLKzrKbWMGap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"Regression R^2 coefficient: {0:2f}\".format(r2_score(y, prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiZQKei0MGaq",
        "colab_type": "text"
      },
      "source": [
        "Clearly, a simple linear regression isn't complex enough to generate useful predictions. More complex ML algorithms are needed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR3Wj8rlMGar",
        "colab_type": "text"
      },
      "source": [
        "Relationship between drv_sex1 and claim_nb:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42nNLBJ-MGas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data.groupby(\"drv_sex1\")[\"claim_nb\"].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5iL4acLMGau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data[final_data['claim_nb'] > 0].groupby(\"drv_sex1\")[\"claim_nb\"].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRnmBBLLMGaw",
        "colab_type": "text"
      },
      "source": [
        "Relationship between pol_coverage and claim_amount:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZzkdBCjMGax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data.groupby(\"pol_coverage\")[\"claim_amount\"].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HDHkeH9MGaz",
        "colab_type": "text"
      },
      "source": [
        "Relationship between pol_coverage and claim_amount:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If7peXbFMGaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data.groupby(\"pol_usage\")[\"claim_amount\"].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_fMfSs6MGa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data.groupby(\"pol_usage\")[\"claim_nb\"].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3QO3qLGMGa2",
        "colab_type": "text"
      },
      "source": [
        "### Part 2) Unsupervised learning\n",
        "In this part, we will analyze the clustering produced by the algorithm K-means. \n",
        "As you've seen this morning, K-means is an unsupervised learning algorithm which aims to partition a dataset of features in K clusters. \n",
        "\n",
        "For computational reasons, we have already pre-computed the clusters using the sklearn library with K = 5 clusters (see https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html if you want to implement it yourself).\n",
        "\n",
        "The file 'Cluster_variables.csv' contains for each cluster, the average value of each feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wdzFuAiMGa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_variable = pd.read_csv(constants.Cluster_variables, header=0)\n",
        "cluster_variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfavnz7wMGa4",
        "colab_type": "text"
      },
      "source": [
        "We generate some plots of interesting features among the different clusters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jqlMQ3eMGa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figure = plt.figure()\n",
        "plt.figure(figsize=(22,30))\n",
        "figure.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "lin_space = np.linspace(1, 5,5)\n",
        "\n",
        "# What's the size of each cluster in terms of number of policy/cluster?\n",
        "plt.subplot(3, 3, 1)\n",
        "plt.bar(lin_space, cluster_variable[\"Size\"])\n",
        "plt.title(\"1) Size (number of policies)\", fontsize=22)\n",
        "plt.xlabel(\"Cluster\", fontsize=22)\n",
        "plt.ylabel(\"Size\", fontsize=22)\n",
        "\n",
        "# What's the average aggregate claim per cluster?\n",
        "plt.subplot(3, 3, 2)\n",
        "plt.bar(lin_space, cluster_variable[\"ag_claims\"])\n",
        "plt.title(\"2) Average claim size\", fontsize=22)\n",
        "plt.xlabel(\"Cluster\", fontsize=22)\n",
        "plt.ylabel(\"Average claim\", fontsize=22)\n",
        "\n",
        "# What's the subscribed proportion to a mileage-based policy, i.e. the premium payed is based on miles driven\n",
        "plt.subplot(3, 3, 3)\n",
        "plt.bar(lin_space, cluster_variable[\"pol_payd(1=Yes)\"])\n",
        "plt.title(\"3) Subscribed prop. to a mileage-based policy\", fontsize=22)\n",
        "plt.xlabel(\"Cluster\", fontsize=22)\n",
        "plt.ylabel(\"Proportion\", fontsize=22)\n",
        "\n",
        "# What's the proportion of retiree?\n",
        "plt.subplot(3, 3, 4)\n",
        "plt.bar(lin_space, cluster_variable[\"pol_usage(Retired)\"])\n",
        "plt.title(\"4) Retiree proportion\", fontsize=22)\n",
        "plt.xlabel(\"Cluster\", fontsize=22)\n",
        "plt.ylabel(\"Proportion\", fontsize=22)\n",
        "\n",
        "# What's the coverage distribution of the insurance portfolio?\n",
        "plt.subplot(3, 3, 5)\n",
        "plt.bar(lin_space, cluster_variable[\"pol_usage(Professional)\"])\n",
        "plt.title(\"5) Professional proportion\", fontsize=22)\n",
        "plt.xlabel(\"Cluster\", fontsize=22)\n",
        "plt.ylabel(\"Proportion\", fontsize=22)\n",
        "\n",
        "# What's the population density?\n",
        "plt.subplot(3, 3, 6)\n",
        "plt.bar(lin_space, cluster_variable[\"dens_pop\"])\n",
        "plt.title(\"6) Population density\", fontsize=22)\n",
        "plt.xlabel(\"Cluster\", fontsize=22)\n",
        "plt.ylabel(\"Density\", fontsize=22)\n",
        "\n",
        "# What's the proportion of Male as first driver?\n",
        "plt.subplot(3, 3, 7)\n",
        "plt.bar(lin_space, cluster_variable[\"drv_sex1(1=Male)\"])\n",
        "plt.title(\"7) Proportion of Male as first driver\", fontsize=22)\n",
        "plt.xlabel(\"Cluster\", fontsize=22)\n",
        "plt.ylabel(\"Proportion\", fontsize=22)\n",
        "\n",
        "# What's the vehicle value?\n",
        "plt.subplot(3, 3, 8)\n",
        "plt.bar(lin_space, cluster_variable[\"vh_value\"])\n",
        "plt.title(\"8) Vehicle value distribution\", fontsize=22)\n",
        "plt.xlabel(\"Cluster\", fontsize=22)\n",
        "plt.ylabel(\"Value\", fontsize=22)\n",
        "\n",
        "# What's the vehicule speed?\n",
        "plt.subplot(3, 3, 9)\n",
        "plt.bar(lin_space, cluster_variable[\"vh_speed\"])\n",
        "plt.title(\"9) Vehicule speed distribution\", fontsize=22)\n",
        "plt.xlabel(\"Cluster\", fontsize=22)\n",
        "plt.ylabel(\"Speed\", fontsize=22)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZtY5EhcMGa6",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1) Analysis of the resulting clusters\n",
        "- Based on the 9 plots shown before, which cluster of policies should be considered the lowest/highest risk cluster?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLXRqVPaMGa6",
        "colab_type": "text"
      },
      "source": [
        "#### Analysis of the lowest risk cluster: number 3.\n",
        "- Average aggregate claim: smallest by a large margin (Plot 2).\n",
        "- Average car value: by far the smallest (Plot 8). \n",
        "- Car speed: slower on average (Plot 9).\n",
        "- Population density: significantly smaller, which implies less risk (Plot 6).  \n",
        "- Male first driver: significantly less than the other clusters (Plot 7).\n",
        "- Policy usage: more 'Retirees' and less 'Professionals' in this cluster (Plot 4 and 5).\n",
        "- Mileage-based policy subscription: a bit higher than the other clusters (Plot 3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni7v_PmRMGa7",
        "colab_type": "text"
      },
      "source": [
        "#### Analysis of the highest risk cluster: number 5.\n",
        "- Average aggregate claim: highest by a large margin (Plot 2).\n",
        "- Average car value: by far the highest (Plot 8). \n",
        "- Car speed: faster on average (Plot 9).\n",
        "- Male first driver: significantly more male as the first driver (Plot 7).\n",
        "- Population density: on the higher end, smaller than cluster 4 (see below for a comment on cluster 4) (Plot 6). \n",
        "- Policy usage: significantly more 'Professionals' (Plot 5).\n",
        "- Mileage-based policy subscription: on the lower end (Plot 3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGoGc72FMGa8",
        "colab_type": "text"
      },
      "source": [
        "#### Additional notes:\n",
        "- Cluster 4: the density population is by far the largest amongst the 5 clusters (Plot 6). If you look at the INSEE table of this cluster, almost everyone lives in the department 75, which is...Paris!\n",
        "- Cluster 3: contains the largest number of policies, thus we can have a high level of confidence in the conclusion we've made..\n",
        "- Cluster 5: contains the smallest number of policies by far (less than 2% of the policies). Thus, in practice, we would advocate keeping in mind that the number of policies is small before drawing conclusions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toWU1wpAMGa8",
        "colab_type": "text"
      },
      "source": [
        "### Part 3: Introduction to the deep learning library Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dec06IliMGa8",
        "colab_type": "text"
      },
      "source": [
        "#### Goals:\n",
        "- A) Introduction of the Keras library on a high-level for supervised learning tasks.\n",
        "- B) On a toy example (iris dataset), predict the type of flower of the iris dataset (classification) as well as a regression task with a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDuulyHMGa9",
        "colab_type": "text"
      },
      "source": [
        "A) Importation of the Keras library\n",
        "- Keras is a high-level deep learning library that is considered to be user-friendly, modular and extensible. \n",
        "- Keras can be used to compile different types of deep neural networks which you will see throughout this week: multilayer perceptron (today), convolutional layer, recurrent layer (tomorrow), etc...\n",
        "- The implementation of a neural network with Keras requires a small number of lines of code. The training part (estimation of parameters) is also optimized such that it can be done automatically for you (more details about this tomorrow, see stochastic gradient descent and backprop algorithm). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5eyuY82MGa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential, load_model      \n",
        "from keras.layers import Dense       # 'Dense' layer is a fully-connected layer in Keras, i.e. the layer used in an MLP \n",
        "from keras.utils import np_utils     # For the one-hot encoding, see below. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVuApelzMGbA",
        "colab_type": "text"
      },
      "source": [
        "#### B.1.1) First supervised learning task\n",
        "- We will use a toy example for computational reasons: the Iris dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVD0Bmd7MGbA",
        "colab_type": "text"
      },
      "source": [
        "Let's first download the iris dataset\n",
        "- The iris dataset is really simple. It consist of 150 flowers with 4 features: petal length, petal width, sepal length, sepal width and three species of flowers (setosa, virginica and versicolor). \n",
        "- We will try to predict the type of flower based on the 4 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuAeoADXMGbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris_data = load_iris() \n",
        "features = iris_data['data']\n",
        "targets = iris_data['target']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KQdOC96MGbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"examples of features:\")\n",
        "print (features[0:10,:])\n",
        "print(\"species\")\n",
        "print(targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAnRgbgaMGbE",
        "colab_type": "text"
      },
      "source": [
        "For a multi-classification task, it's best practice to encode the targets as a 'one-hot' encoding. \n",
        "- A one-hot incoding is simply a vector of zeros everywhere except for a '1' at the position of the class. \n",
        "- Ex: for the iris dataset, we have three classes. A class '0' will be encoded as :[1., 0, 0], a class '1': [0,1,0] and class '2': [0,0,1]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkGC78qfMGbE",
        "colab_type": "text"
      },
      "source": [
        "One-hot encoding is done in the following box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OByEnwIvMGbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_y = np_utils.to_categorical(targets)\n",
        "print(dummy_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrTZepCuMGbG",
        "colab_type": "text"
      },
      "source": [
        "Let's split our dataset into a Train|Test set of 70%/30%. \n",
        "- This is done with the function 'train_test_split'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEV0FVg7MGbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(features, dummy_y, test_size=0.3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id_D0WqsMGbI",
        "colab_type": "text"
      },
      "source": [
        "Let's compile our first neural network model to predict the type of flower.\n",
        "- Single hidden layer of 5 neurons each with activation function 'tanh'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "XhY8ClMJMGbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Input layer: 5 hidden neurons and 'tanh' activation function\n",
        "model.add(Dense(units=5, activation='tanh', input_dim=x_train.shape[-1]))\n",
        "\n",
        "# Output layer: need to use softmax as we have a classification problem\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model by specifying the loss function (cross entropy), the optimizer (Adam) and optional metric to output ('accuracy')\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0pHWfVdMGbL",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the performance on the Train|Test set from the resulting neural network\n",
        "- In the context of multi-classification, the function 'predict' of Keras takes as input a set of features, and outputs the probability of being in each class.\n",
        "- i.e. model.predict(x_train) outputs a discrete probability distribution over the 3 types of flowers for each example in our training set. \n",
        "- The predicted flower will be the highest probability class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E43mss9WMGbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_train = model.predict(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf1Z5OdgMGbO",
        "colab_type": "text"
      },
      "source": [
        "Let's analyze the predicted discrete probability distribution for the first 5 examples in the train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIG_GYexMGbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Probability distribution for the first 5 examples of the Train set:\")\n",
        "print(y_pred_train[0:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AEnLsQfMGbQ",
        "colab_type": "text"
      },
      "source": [
        "and the predicted class for the same five examples vs the true label (true type of flower). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCe3IdsoMGbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Predicted flower for the first 5 examples (goes from 0 to 2):\")\n",
        "print(np.argmax(y_pred_train[0:5],axis=1))\n",
        "print(\"True type of flowers for the first 5 examples\")\n",
        "print(np.argmax(y_train[0:5],axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcEAalE9MGbT",
        "colab_type": "text"
      },
      "source": [
        "Let's compute the accuracy on the train and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mzMO7-AMGbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Train set accuracy:\")\n",
        "print(np.sum(np.argmax(y_pred_train, axis=1)==np.argmax(y_train, axis=1))/x_train.shape[0])\n",
        "print(\"Number of errors on the Train set out of %d examples:\" %(x_train.shape[0]))\n",
        "print(x_train.shape[0] - np.sum(np.argmax(y_pred_train, axis=1)==np.argmax(y_train, axis=1)))\n",
        "\n",
        "print(\"Test set accuracy:\")\n",
        "y_pred_test = model.predict(x_test)\n",
        "print(np.sum(np.argmax(y_pred_test, axis=1)==np.argmax(y_test, axis=1))/x_test.shape[0])\n",
        "print(\"Number of errors on the Test set out of %d examples:\" %(x_test.shape[0]))\n",
        "print(x_test.shape[0] - np.sum(np.argmax(y_pred_test, axis=1)==np.argmax(y_test, axis=1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rGPbJ6JMGbU",
        "colab_type": "text"
      },
      "source": [
        "#### B.1.2) Predict the sepal width based on the other features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUtzXQuBMGbV",
        "colab_type": "text"
      },
      "source": [
        "Still with the 'iris' dataset, let's predict the value of the 'sepal width' (the 4th feature) based on the other three other. \n",
        "- This is a regression task, as the four features take real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHmutMtmMGbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_without_sepal_width = features[:,:-1]\n",
        "sepal_width = features[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QBaaN__MGbW",
        "colab_type": "text"
      },
      "source": [
        "Again, let's split our dataset into a Train|Test set of 70%. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBAQkpbUMGbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(features_without_sepal_width, sepal_width, test_size=0.3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkLmyjpsMGbY",
        "colab_type": "text"
      },
      "source": [
        "Let's compile our neural network. Notice two important differences:\n",
        "- 1) Output layer: must be of size 1 and the activation is no longer 'softmax', it's a 'linear' activation function.\n",
        "- 2) Loss function: mean-square error (MSE) instead of cross-entropy. MSE is the default loss function to use in most regression tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dhyfJu3aMGbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Input layer: 5 hidden neurons and 'tanh' activation function\n",
        "model.add(Dense(units=5, activation='tanh', input_dim=x_train.shape[-1]))\n",
        "\n",
        "# Output layer: default activation function is 'linear'\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model by specifying the loss function (MSE), the optimizer (Adam).\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(x_train,y_train,epochs=50,batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cJ7tP68MGba",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the performance on the Train|Test set from the resulting neural network.\n",
        "- In the context of a regression task, the function 'predict' of Keras takes as input a set of features, and outputs a scalar that represents the prediction of the target (sepal width)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPP-FH9pMGbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_train = model.predict(x_train)\n",
        "print(\"Predicted sepal width for the first 5 examples\")\n",
        "print(y_pred_train[0:5])\n",
        "print(\"True sepal width for the first 5 examples\")\n",
        "print(y_train[0:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddv_iJbFMGbd",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the total performance on the Train and Test set of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbBsNyjoMGbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Mean-square error obtained on the Train set:\")\n",
        "print(np.average((y_pred_train[:,0]-y_train)**2))\n",
        "\n",
        "print(\"Mean-square error obtained on the Test set:\")\n",
        "y_pred_test = model.predict(x_test)\n",
        "print(np.average((y_pred_test[:,0]-y_test)**2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw1gAPGZMGbf",
        "colab_type": "text"
      },
      "source": [
        "## Part 4: Hyperparameter search with two popular methods: grid search and random search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9LotgfYMGbg",
        "colab_type": "text"
      },
      "source": [
        "### 4.1) Grid search algorithm for model tuning and study of learning curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdoVkFAPMGbh",
        "colab_type": "text"
      },
      "source": [
        "- A) Search over the following simple grid (each combination will be tested):\n",
        "    - {1,2} layers\n",
        "    - {5,10,15,20} neurons/layer\n",
        "    - {'relu', 'sigmoid', 'tanh'} activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oi2C4JJMGbh",
        "colab_type": "text"
      },
      "source": [
        "For the rest of this notebook, we will work on the classification task of the iris dataset (i.e. predict the type of flower). \n",
        "- Note: grid search and random search are applicable to any machine learning task that requires tuning of the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPBpCUPTMGbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1) Reload the dataset\n",
        "iris_data = load_iris() \n",
        "features = iris_data['data']\n",
        "targets = iris_data['target']\n",
        "\n",
        "# 2) one-hot encoding of the targets\n",
        "dummy_y = np_utils.to_categorical(targets)\n",
        "\n",
        "# 3) Split 70%|30% for train and validation set\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(features, dummy_y, test_size=0.3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51WKso4mMGbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "# Earlystopping criteria: if after 10 epochs, the validation loss ('val_loss') \n",
        "# has not improved by at least 0.001: \n",
        "# - stop, and restore the best weights (i.e. 5 epochs before). \n",
        "\n",
        "nb_epoch = 100\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5,\n",
        "                          verbose=1, mode='auto', restore_best_weights=True)\n",
        "\n",
        "# Grid search\n",
        "nbs_layers_range = np.array([1,2])               # 1 or 2 hidden layers\n",
        "nbs_neurons_range = np.array([5,10,15,20])       # for each hidden layer, either {100,120,140} hidden neurons\n",
        "activation_range = ['relu', 'sigmoid', 'tanh']   \n",
        "\n",
        "# Statistics to keep track of during the optimization\n",
        "valid_loss_best = 99999999\n",
        "best_nbs_layer = 999999999\n",
        "best_nbs_neurons = 9999999\n",
        "best_batch_size = 99999999"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vYGV0OzMMGbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loop over the different combination on the grid\n",
        "for i in range(len(nbs_layers_range)):\n",
        "    nbs_layer = nbs_layers_range[i]\n",
        "    for j in range(len(nbs_neurons_range)):\n",
        "        nbs_neurons = nbs_neurons_range[j]\n",
        "        for k in range(len(activation_range)):\n",
        "            activation = activation_range[k]\n",
        "            \n",
        "            print(\"Current set of HP: %d hidden layers, %d number of neurons, %s activation\" %(nbs_layer, nbs_neurons, activation))\n",
        "            \n",
        "            # Compile the model:\n",
        "            model = Sequential()\n",
        "            \n",
        "            # First hidden layer\n",
        "            model.add(Dense(units = nbs_neurons, activation = activation, input_dim = x_train.shape[1]))\n",
        "            \n",
        "            # Check if we have to add a second hidden layer on top of the first one\n",
        "            if(nbs_layer == 2):\n",
        "                model.add(Dense(units = nbs_neurons, activation = activation))\n",
        "            \n",
        "            # Output layer\n",
        "            model.add(Dense(3, activation='softmax'))\n",
        "            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "             \n",
        "            # With the given set of hyperparameters, train the model \n",
        "            model.fit(x_train, y_train, epochs=nb_epoch, batch_size=10, verbose = 1,  \n",
        "                      validation_data=(x_valid, y_valid), callbacks=[earlystop])\n",
        "            \n",
        "            # Evaluate the validation loss of the trained model\n",
        "            [val_loss, val_accu] = model.evaluate(x_valid, y_valid)\n",
        "            \n",
        "            # If it's the best model so far:\n",
        "            # - Save the model and update the statistics\n",
        "            if (val_loss < valid_loss_best):\n",
        "                valid_loss_best = val_loss\n",
        "                best_nbs_layer = nbs_layer\n",
        "                best_nbs_neurons = nbs_neurons\n",
        "                best_activation = activation\n",
        "                #model.save(\"best_model_grid_search.h5\")\n",
        "\n",
        "# Print the resulting best model:\n",
        "print(\"Best results obtained:\")\n",
        "print(\"Best number of neurons: %d\" %(best_nbs_neurons))\n",
        "print(\"Best nbs layers: %d\" %(best_nbs_layer))\n",
        "print(\"Best activation layer: %s\" %(best_activation))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYrNfRv1MGbm",
        "colab_type": "text"
      },
      "source": [
        "Some notes:\n",
        "- Best resulting model we got:\n",
        "    - Number of neurons: 20\n",
        "    - Nbs layers: 2\n",
        "    - Activation function: tanh\n",
        "- Even with our toy example (complete dataset of 150 examples) and a small grid of hyperparameter (24 combinations), the optimization with Grid Search takes a significant amount of time;\n",
        "- Random search (next method) is known to be computationally much more efficient than Grid Search.\n",
        "    - Does not mean that Random search will provide a better set of hyperparameters than Grid Search;\n",
        "    - Means that on average, for a given fix computational cost, Random search will provide a better set of hyperparameters than Grid search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqZ-i00VMGbn",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.1) Compute the learning curves of the best model found by Grid Search\n",
        "- Need to retrain our model with best set of hyper-parameters if we want to monitor the learning curves.\n",
        "- Note: the computation of the learning curves could have been done simultaneously with Grid Search \n",
        "    - but it would be suboptimal;\n",
        "    - Reason: we would need to compute the learning curves for EACH combination of hyperparameters tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "F6h1STwHMGbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1) Compile the model with the best set of HP's we got when we ran the code:\n",
        "#    - Number of neurons: 20\n",
        "#    - Nbs layers: 2\n",
        "#    - Activation function: tanh\n",
        "import random\n",
        "random.seed(30)\n",
        "model_lc_grid_search = Sequential()\n",
        "model_lc_grid_search.add(Dense(units = 20, activation = 'tanh', input_dim = x_train.shape[1]))\n",
        "model_lc_grid_search.add(Dense(units = 20, activation = 'tanh'))\n",
        "model_lc_grid_search.add(Dense(3, activation='softmax'))\n",
        "model_lc_grid_search.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 2) Train the model over 100 epochs, 1 epoch at a time. \n",
        "# - At the end of each epoch, monitor the losses on each dataset (train|valid|test)\n",
        "nb_epoch = 300\n",
        "batch_size = 10\n",
        "results_tensor = np.zeros((nb_epoch,2, 2))  # Compile the loss and accuracy on the Train|Valid after each epoch\n",
        "\n",
        "# Fit the model 1 epoch at a time. \n",
        "for i in range(nb_epoch):\n",
        "    model_lc_grid_search.fit(x_train, y_train, epochs=1, batch_size=batch_size,verbose=1)\n",
        "    \n",
        "    # note: model_lc_grid_search.evaluate returns a list of values: [loss, accuracy]\n",
        "    results_tensor[i,0,:] = model_lc_grid_search.evaluate(x_train, y_train, verbose = 0)  # train set computation\n",
        "    results_tensor[i,1,:] = model_lc_grid_search.evaluate(x_valid, y_valid, verbose = 0)  # valid set computation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCumHV4SPyKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1) Accuracy plots\n",
        "lin_nb_epoch = np.linspace(1,300,300)\n",
        "fig = plt.figure(figsize=(5, 5), dpi=100)\n",
        "plt.plot(lin_nb_epoch, results_tensor[:,0,1], lin_nb_epoch, results_tensor[:,1,1])\n",
        "plt.title('Figure 1 - Accuracy - Grid Search')\n",
        "plt.legend(['Train', 'Valid'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "# 2) Loss plots\n",
        "fig = plt.figure(figsize=(5, 5), dpi=100)\n",
        "plt.plot(lin_nb_epoch, results_tensor[:,0,0], lin_nb_epoch, results_tensor[:,1,0])\n",
        "plt.title('Figure 2 - Loss - Grid Search')\n",
        "plt.legend(['Train', 'Valid'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZjTcGd7MGbp",
        "colab_type": "text"
      },
      "source": [
        "### 4.2) Random search algorithm for model tuning\n",
        "- A) Define a space of possible HPs from which we want to find an optimal set.\n",
        "- B) Random search: \n",
        "    - For a fix number of trials, sample randomly a set of HPs from the defined space;\n",
        "    - Choose the set of HPs which minimizes the MSE on the valid set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoVK0-loMGbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_epoch = 100\n",
        "batch_size = 5\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=15,\n",
        "                          verbose=1, mode='auto', restore_best_weights=True)\n",
        "\n",
        "# Search space - define the boundaries for each HP\n",
        "nbs_layers_range = np.array([1,2,3])             # 1 or 2 hidden layers\n",
        "nbs_neurons_range = np.array([5,30])             # number of neurons within {5,6,...,20}\n",
        "activation_range = ['relu', 'sigmoid', 'tanh']   \n",
        "lr_range = np.array([0.0001,0.01])               # learning rate within [0.0001, 0.01]\n",
        "\n",
        "# Statistics to keep track of during the optimization\n",
        "valid_loss_best = 999999\n",
        "best_nbs_layer = 99999\n",
        "best_nbs_neurons = 99999\n",
        "best_lr = 99999\n",
        "\n",
        "# Number of iterations to be done in the random search\n",
        "nbs_iteration = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AJsR1uyOMGbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loop over the number of iterations\n",
        "for i in range(nbs_iteration):\n",
        "    \n",
        "    # 1) At the beginning of each iteration, randomly sample a set of HP's\n",
        "    nbs_layers = np.random.randint(low=nbs_layers_range[0], high=nbs_layers_range[1]+1)\n",
        "    nbs_neurons = np.random.randint(low=nbs_neurons_range[0], high=nbs_neurons_range[1]+1)\n",
        "    acti_idx = np.random.randint(low=0, high=len(activation_range))\n",
        "    activation = activation_range[acti_idx]\n",
        "    learning_rate = np.random.uniform(low=lr_range[0], high=lr_range[1])\n",
        "    print(\"Iteration %d --- Nbs layers: %d, Nbs neurons: %d, learning rate: %.4f, activation function: %s\" % (i+1, nbs_layers, nbs_neurons, learning_rate, activation))\n",
        "    \n",
        "    # Compile the model:\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units = nbs_neurons, activation = activation, input_dim = x_train.shape[1]))\n",
        "            \n",
        "    # Check if we have to add a third hidden layer on top of the first one\n",
        "    if(nbs_layer == 2):\n",
        "        model.add(Dense(units = nbs_neurons, activation = activation))\n",
        "            \n",
        "    # Output layer\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "             \n",
        "    # With the given set of hyperparameters, train the model \n",
        "    model.fit(x_train, y_train, epochs=nb_epoch, batch_size=10, verbose = 1,  \n",
        "            validation_data=(x_valid, y_valid), callbacks=[earlystop])\n",
        "            \n",
        "    # Evaluate the validation loss of the trained model\n",
        "    [val_loss, val_accu] = model.evaluate(x_valid, y_valid)\n",
        "            \n",
        "    # If it's the best model so far:\n",
        "    # - Save the model and update the statistics\n",
        "    if (val_loss < valid_loss_best):\n",
        "        valid_loss_best = val_loss\n",
        "        best_nbs_layer = nbs_layer\n",
        "        best_nbs_neurons = nbs_neurons\n",
        "        best_activation = activation\n",
        "        best_lr = learning_rate\n",
        "        #model.save(\"best_model_random_search.h5\")\n",
        "        \n",
        "# Print the resulting best model:\n",
        "print(\"Best results obtained:\")\n",
        "print(\"Best number of neurons: %d\" %(best_nbs_neurons))\n",
        "print(\"Best nbs layers: %d\" %(best_nbs_layer))\n",
        "print(\"Best activation layer: %s\" %(best_activation))\n",
        "print(\"Best learning rate: %.4f\" %(best_lr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVqL0RRuMGbs",
        "colab_type": "text"
      },
      "source": [
        "### 4.3) Train|Valid|Test MSE for grid search and random search\n",
        "- For this last part, we will upload the models we have already optimized using grid search and random search;\n",
        "- For grid search, we got:\n",
        "    - Best number of neurons: 20\n",
        "    - Best nbs layers: 2\n",
        "    - Best activation layer: tanh\n",
        "- For random search, we got:\n",
        "    - Best number of neurons: 30\n",
        "    - Best nbs layers: 2\n",
        "    - Best activation layer: tanh\n",
        "    - Best learning rate: 0.0043"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXEh0ufjMGbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the pre-run best models\n",
        "class Constants:\n",
        "    \n",
        "    SEED = 1\n",
        "\n",
        "    # Paths to pre-ran models\n",
        "    best_model_grid_search_final = '/best_model_grid_search_final.h5'\n",
        "    best_model_random_search_final = '/best_model_random_search_final.h5'\n",
        "    \n",
        "    # Google drive id to be able to download from drive\n",
        "    best_model_grid_search_final_ID = '17_f7xutqw0GwYRC-hJ9-1C9UVk5iL3q3'\n",
        "    best_model_random_search_final_ID = '1913m7CWOvO4c2BBCLyvdoPImE0xCQeCi'\n",
        "\n",
        "constants = Constants\n",
        "random.seed(constants.SEED)\n",
        "\n",
        "!rm /best_model_grid_search_final.h5\n",
        "!rm /best_model_random_search_final.h5\n",
        "\n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id=constants.best_model_grid_search_final_ID, dest_path=constants.best_model_grid_search_final, unzip=False)\n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id=constants.best_model_random_search_final_ID, dest_path=constants.best_model_random_search_final, unzip=False)\n",
        "\n",
        "# Test to see everything is working as intended\n",
        "from keras.models import load_model\n",
        "best_model_grid_search_final = load_model(constants.best_model_grid_search_final)\n",
        "best_model_random_search_final = load_model(constants.best_model_random_search_final)\n",
        "\n",
        "print(\"Model found by random search:\")\n",
        "best_model_random_search_final.summary()\n",
        "print(\"-----------------------------\")\n",
        "print(\"Model found by grid search:\")\n",
        "best_model_grid_search_final.summary()\n",
        "print(\"-----------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "CRDvNm9XMGbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1) Compute the Train set loss and accuracy with each model\n",
        "[loss_train_random_search, acc_train_random_search] = best_model_random_search_final.evaluate(x_train,y_train, verbose=0)\n",
        "[loss_train_grid, acc_train_grid] = best_model_grid_search_final.evaluate(x_train,y_train, verbose=0)\n",
        "\n",
        "print(\"Loss Train set\")\n",
        "print(\"Random search: %.4f\" % loss_train_random_search)\n",
        "print(\"Grid search: %.4f\" % loss_train_grid)\n",
        "print(\"------------------------------------------\")\n",
        "print(\"Accuracy Train set\")\n",
        "print(\"Random search: %.4f\" % acc_train_random_search)\n",
        "print(\"Grid search: %.4f\" % acc_train_grid)\n",
        "print(\"------------------------------------------\")\n",
        "\n",
        "# 2) Compute the Valid set error with each model\n",
        "[loss_valid_random_search, acc_valid_random_search] = best_model_random_search_final.evaluate(x_valid,y_valid, verbose=0)\n",
        "[loss_valid_grid, acc_valid_grid] = best_model_grid_search_final.evaluate(x_valid,y_valid, verbose=0)\n",
        "\n",
        "print(\"Loss Valid set\")\n",
        "print(\"Random search: %.4f\" % loss_valid_random_search)\n",
        "print(\"Grid search: %.4f\" % loss_valid_grid)\n",
        "print(\"------------------------------------------\")\n",
        "print(\"Accuracy Valid set\")\n",
        "print(\"Random search: %.4f\" % acc_valid_random_search)\n",
        "print(\"Grid search: %.4f\" % acc_valid_grid)\n",
        "print(\"------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF5kf2dQMGbw",
        "colab_type": "text"
      },
      "source": [
        "Conclusion:\n",
        "- 1) In general, Random search > Grid search because of the computational burden from Grid Search. \n",
        "- 2) The optimization procedures shown in this notebook (grid search and random search) are applicable in many machine learning algorithms. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb3RRIacSa2B",
        "colab_type": "text"
      },
      "source": [
        "### Part 5) Insurance classification ('homework')\n",
        "- Based on what you've learned in Keras and multiclass classification, build a classification MLP that predicts the car brand ['vh_make'] given its characteristics (features).\n",
        "- Note:\n",
        "    - Total number of vehicule makers: 101\n",
        "    - Top 5: Renault, peugeot, citroen, volkswagen, ford with a respective proportion of [26.8%, 19.6%, 15.9%, 5.3%, 4.5%]\n",
        "    - Other 96 brands cumulative proportion: 28%\n",
        "- Conclusion:\n",
        "    - A) Good practice: try to tackle one part of the problem if the problem seems very complex to validate if it's possible to solve:\n",
        "        - i.e.: build an MLP to classify a car as either [Renault, peugeot, citroen, volkswagen, ford, others]. \n",
        "    - B) If part A) works, try to generalize your model to more vehicules. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFhKpyX6SdrX",
        "colab_type": "text"
      },
      "source": [
        "Here are some suggestions for you to follow:\n",
        "- 1) Split the insurance dataset 'clean_data' into a set of features and targets:\n",
        "    - Targets: one-hot encoding of the possible car brand \n",
        "        - Hint: there's a one-line function to do this (see what we've done previously with the Iris dataset).  \n",
        "    - Features: Your choice.\n",
        "        - Don't necessarily need to use all of them.\n",
        "    \n",
        "- 2) Split the resulting dataset into a Train|Valid sets\n",
        "    - Hint: one line code. \n",
        "\n",
        "- 3) Try a simple neural network (no hyperparameter search):\n",
        "    - This is to validate that the task is possible to solve;\n",
        "    - For ex: 2 layers, 50 neurons/layer with relu activation function, could it potentially work?\n",
        "    - For the neural network implementation: look at the multiclassification task implemented on the Iris dataset. \n",
        "    \n",
        "- 4) If the implementation in step 3) shows potential, use a hyperparameter search to optimize the results:\n",
        "    - Suggestion: only try Random Search. \n",
        "    \n",
        "- 5) If step 4) shows potential, try a more complex task by adding more possible classes (i.e. more car brand!)"
      ]
    }
  ]
}