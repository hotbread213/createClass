{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "7AmobHpaNxeW",
        "J6Lf_hjvurNk",
        "DElMZCH-k5Qd"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hotbread213/createClass/blob/master/day1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hefHrubzgBup",
        "colab_type": "text"
      },
      "source": [
        "# Fin-ML/IVADO Finance and Insurance Workshop\n",
        "## Introduction to Machine Learning\n",
        "Day 1 afternoon tutorial, March 4th 2019\n",
        "\n",
        "1. Intro to Numpy and Scipy\n",
        "\n",
        "2. Intro to Scikit-learn and machine learning concepts\n",
        "\n",
        "  * Supervised learning (both frequentist and Bayesian)\n",
        "    * Fitting and prediction\n",
        "    * Generalization error\n",
        "    * Regularization and priors\n",
        "    * Hyperparameter tuning and model selection\n",
        "    * Exercise\n",
        "  * Unsupervised learning\n",
        "    * A clustering example\n",
        " \n",
        "3.  Intro to Pytorch\n",
        "\n",
        "  * A multilayer perceptron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duZfqKmElVgh",
        "colab_type": "text"
      },
      "source": [
        "## 1. Intro to Numpy and Scipy\n",
        "\n",
        "The Python standard library has only limited support for computational mathematics, mostly in `math` and `random`. To remedy these deficiencies, by far the most popular libraries are `numpy` and `scipy`.\n",
        "\n",
        "**`numpy`** is a tensor algebra library. Typical operations supported by `numpy` include tensor multiplication, indexing, random sampling, ect. Most routines are ultimately a wrapper around BLAS/LAPACK implementations in Fortran.\n",
        "\n",
        "**`scipy`** is a computational mathematics library that complements `numpy`. Typical operations supported by `scipy` include \"higher-level\" numerical algebra (such as solving linear systems), numerical integration, Fourier transforms, ect.\n",
        "\n",
        "Those are not specifically _machine learning_ libraries, but they are often used as the foundation for machine learning code, so it is essential (and inescapable) to learn to use them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md2fGFzLO--h",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Insurance data\n",
        "\n",
        "Now that we have upgraded numpy, we are ready to do some data analysis.\n",
        "As a motivating example throughout the afternoon, we will use an insurance dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBNZw87cSnGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O /content/insurance_data.pickle https://www.dropbox.com/s/8enthlp3q4mxtdx/insurance_data.pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgBBEgX4pl9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pickle\n",
        "\n",
        "with open(\"/content/insurance_data.pickle\", 'rb') as file:\n",
        "  data = pickle.load(file)\n",
        "  \n",
        "print(f\"The dataset has {data['features'].shape[0]} examples with {data['features'].shape[1]} features\\n\")\n",
        "\n",
        "print(f\"First five features: {data['feature_names'][:5]}\")\n",
        "print(f\"First five features of first observation: {data['features'][0, :5]}\")\n",
        "print(f\"Total claims of first observation: {data['total_claims'][0]}$\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "motJrU-Fzdo8",
        "colab_type": "text"
      },
      "source": [
        "Tensor algebra works rather intuitively in numpy. For example, if we wanted to create a binary vector that encodes whether a given policy had a nonzero claim, we could do:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LpQwhj4zuBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['has_claim'] = (data['total_claims'] != 0)\n",
        "nb_of_nonzero_claims = sum(data['has_claim'])\n",
        "print(f\"There are {nb_of_nonzero_claims} policies with nonzero claims among the {len(data['has_claim'])}.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC7QZn4C0Mtp",
        "colab_type": "text"
      },
      "source": [
        "We can also easily index. For example, to access the subset of all policies that have nonzero claims, we can do:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azHHv6yq0L72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['features_with_claims'] = data['features'][data['has_claim'], :]\n",
        "data['nonzero_claims'] = data['total_claims'][data['has_claim']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cbwl9mEt0_l",
        "colab_type": "text"
      },
      "source": [
        "## 2. Intro to Machine Learning and Scikit-Learn\n",
        "\n",
        "We are now ready to do actual machine learning!\n",
        "\n",
        "**`scikit-learn`** is a general-purpose library with implementations of \"classic\" machine learning models. It also offers a variety of utilities that can help with preprocessing.\n",
        "\n",
        "In this tutorial we won't dive into the details of the models we will be using. Instead, we'll discuss and compute key machine learning concepts. They don't depend on the particular model chosen, so we might as well treat the models as black boxes for now, which people often do anyways. The next days will cover in more detail how some of these models work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw3LhpkPugsw",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Supervised learning\n",
        "\n",
        "As you saw this morning, in supervised learning you are given feature-response pairs $(x, y)$ and you must learn the mapping $f(x)=y$.\n",
        "\n",
        "There are two main approaches for learning such a function: the frequentist and the Bayesian way. In both cases however, the goal is the same, namely to use the training examples to predict as well as possible new, unseen examples. That is, the key goal is **generalization**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hGu-82u19LH",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1.1 Fitting and prediction\n",
        "\n",
        "To demonstrate these concepts, let's fit a classifier to predict whether a given policy will have a nonzero claim, say on the first 1,000 claims so that training does not take too long. We will use a Bayesian model called a **Gaussian process** to do so.\n",
        "\n",
        "Let's first create the model and prepare the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4ubfLHEujaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "nonzero_claim_model = GaussianProcessClassifier()\n",
        "\n",
        "features = data['features'][:1000, :]\n",
        "responses = data['has_claim'][:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQVie3AH5L4B",
        "colab_type": "text"
      },
      "source": [
        "Let's now fit the model. What kind of accuracy do we get after training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxxcigrW3MUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nonzero_claim_model.fit(features, responses)\n",
        "\n",
        "accuracy = nonzero_claim_model.score(features, responses)\n",
        "print(f\"I am able to predict with {100*accuracy:.2f}% accuracy if a policy will have a claim.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNV3amyZ7G7b",
        "colab_type": "text"
      },
      "source": [
        "Wow!! That's **very** impressive!\n",
        "Well, great, we're done here! Let's pack up, we've solved the whole insurance business!\n",
        "\n",
        "...\n",
        "\n",
        "...\n",
        "\n",
        "Or did we?\n",
        "Maybe let's check on the next 1000 policies just to make sure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUhjD0w38RBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next_1000_features = data['features'][1000:2000, :]\n",
        "next_1000_responses = data['has_claim'][1000:2000]\n",
        "\n",
        "next_1000_accuracy = nonzero_claim_model.score(next_1000_features, next_1000_responses)\n",
        "print(f\"I am able to predict with {100*next_1000_accuracy:.2f}% accuracy whether one of the next 10000 policies will have a claim.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGuJQ9CK9AMy",
        "colab_type": "text"
      },
      "source": [
        "Wait, the performance dropped... What happened here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2lP4a3z9SD9",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1.2 Generalization error\n",
        "\n",
        "The thing is, it's easy to do well on the data used for training: you could just memorize the labels, for example.\n",
        "The tough part is doing well on data _you haven't seen_. And at this task, not all models are created equal. The problem is, it's not easy picking the right model ahead of time because, well, you don't know on what data you will be assessed, yet!\n",
        "\n",
        "Fortunately, if you have enough data, there is a way to estimate how well you're going to do on a future task: using **train/test splits**. The idea is, take your big dataset, and split it randomly into (say) a 80% train set and a 20% test set. You will train your model on the train set, and assess performance on the test set.\n",
        "\n",
        "Let's try it out for claim prediction. First create a train/test split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiO7qb62_-CV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffled_policies = np.random.permutation(np.arange(len(data['total_claims'])))\n",
        "\n",
        "train_set = shuffled_policies[:8000]\n",
        "test_set = shuffled_policies[-2000:]\n",
        "\n",
        "train_features = data['features'][train_set, :]\n",
        "train_responses = data['has_claim'][train_set]\n",
        "\n",
        "test_features = data['features'][test_set, :]\n",
        "test_responses = data['has_claim'][test_set]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBai5emfA5DI",
        "colab_type": "text"
      },
      "source": [
        "Now let's fit our Gaussian process classifier on the train set. (This will take a minute or two.) What are the train and test accuracies?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZoDUJdjBKnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nonzero_claim_model.fit(train_features, train_responses)\n",
        "\n",
        "train_accuracy = nonzero_claim_model.score(train_features, train_responses)\n",
        "test_accuracy = nonzero_claim_model.score(test_features, test_responses)\n",
        "\n",
        "print(f\"I am able to predict with {100*train_accuracy:.2f}% accuracy if a policy will have a claim on the train set,\"\n",
        "      f\" and with {100*test_accuracy:.2f}% accuracy on the test set.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbJ4QzRiCCqf",
        "colab_type": "text"
      },
      "source": [
        "As you can see, adding more data didn't solve the problem: the problem is the model, not the data.\n",
        "\n",
        "To explore this idea a bit more, let's try to do something a bit more difficult: we're going to fit a **random forest** to predict the size of the claim. Note that this is a regression problem.\n",
        "\n",
        "We'll use the subset of the policies that had nonzero claims for this. Let's first create a train/test split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQFTrYb9DVS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffled_policies = np.random.permutation(np.arange(len(data['nonzero_claims'])))\n",
        "average_claim = np.mean(np.abs(data['nonzero_claims']))\n",
        "\n",
        "train_set = shuffled_policies[:800]\n",
        "test_set = shuffled_policies[800:1000]\n",
        "\n",
        "train_features = data['features_with_claims'][train_set, :]\n",
        "train_responses = data['nonzero_claims'][train_set]\n",
        "\n",
        "test_features = data['features_with_claims'][test_set, :]\n",
        "test_responses = data['nonzero_claims'][test_set]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feBvcioLElc0",
        "colab_type": "text"
      },
      "source": [
        "Let's now fit our random forest to predict the size of the nonzero total claims. What train and test errors do we obtain?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DSZZdybEe6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "claim_size_model = RandomForestRegressor(n_estimators=100)\n",
        "claim_size_model.fit(train_features, train_responses)\n",
        "\n",
        "train_mae = np.mean(np.abs(claim_size_model.predict(train_features) - train_responses))\n",
        "test_mae = np.mean(np.abs(claim_size_model.predict(test_features) - test_responses))\n",
        "print(f\"My predictions are off by ±{train_mae:.0f}$ on the train set on average, and by ±{test_mae:.0f}$ on the test set.\")\n",
        "\n",
        "print(f\"The average claim is {average_claim:.0f}$, so the relative test error is {100*test_mae/average_claim:.2f}%.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA4Rpj0HHhRC",
        "colab_type": "text"
      },
      "source": [
        "Okay, well the prediction quality is not amazing, but it's a good start. Is there something we could do to improve things?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKeF9DyGJt1c",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1.3 Regularization and priors\n",
        "\n",
        "One easy way to improve the (test) performance of a model while overfitting is adding regularization (frequentist case) and using heavier priors (Bayesian case). This tends to reduce the gap between train and test performance, so if the training error is very low, it can reduce the test error. It might also improve the training error.\n",
        "\n",
        "Since random forests are frequentist, we should be adding regularization. In the RF case, this can be achieved by, for example, increasing the minimum number of samples needed for a split.\n",
        "\n",
        "Let's fit such a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzDqPGFd3UUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "claim_size_model = RandomForestRegressor(n_estimators=100, min_samples_split=100)\n",
        "claim_size_model.fit(train_features, train_responses)\n",
        "\n",
        "train_mae = np.mean(np.abs(claim_size_model.predict(train_features) - train_responses))\n",
        "test_mae = np.mean(np.abs(claim_size_model.predict(test_features) - test_responses))\n",
        "print(f\"My predictions are off by ±{train_mae:.0f}$ on the train set on average, and by ±{test_mae:.0f}$ on the test set.\")\n",
        "\n",
        "print(f\"The average claim is {average_claim:.0f}$, so the relative test error is {100*test_mae/average_claim:.2f}%.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYFfVx-UORph",
        "colab_type": "text"
      },
      "source": [
        "Great! This has helped the test loss a bit! It really hurt the training error, but we don't care."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AmobHpaNxeW",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1.4 Hyperparameters tuning and model selection\n",
        "\n",
        "While we're at it, how about varying more hyperparameters? Consider for example removing the bootstrapping of samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BPYhekBaG5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "claim_size_model = RandomForestRegressor(n_estimators=100, bootstrap=False, min_samples_split=100)\n",
        "claim_size_model.fit(train_features, train_responses)\n",
        "\n",
        "train_mae = np.mean(np.abs(claim_size_model.predict(train_features) - train_responses))\n",
        "test_mae = np.mean(np.abs(claim_size_model.predict(test_features) - test_responses))\n",
        "print(f\"My predictions are off by ±{train_mae:.0f}$ on the train set on average, and by ±{test_mae:.0f}$ on the test set.\")\n",
        "\n",
        "print(f\"The average claim is {average_claim:.0f}$, so the relative test error is {100*test_mae/average_claim:.2f}%.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3wU2RGXbmh5",
        "colab_type": "text"
      },
      "source": [
        "Ouch! No, bad idea, that hurt the test error. Okay, how about changing the criterion for MAE?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JAqkGI-Uhah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "claim_size_model = RandomForestRegressor(n_estimators=100, min_samples_split=100, criterion='mae')\n",
        "claim_size_model.fit(train_features, train_responses)\n",
        "\n",
        "train_mae = np.mean(np.abs(claim_size_model.predict(train_features) - train_responses))\n",
        "test_mae = np.mean(np.abs(claim_size_model.predict(test_features) - test_responses))\n",
        "print(f\"My predictions are off by ±{train_mae:.0f}$ on the train set on average, and by ±{test_mae:.0f}$ on the test set.\")\n",
        "\n",
        "print(f\"The average claim is {average_claim:.0f}$, so the relative test error is {100*test_mae/average_claim:.2f}%.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz9Xji0FXYqi",
        "colab_type": "text"
      },
      "source": [
        "Great, that helped the test error! It's a new record performance!\n",
        "\n",
        "In general, we can start playing with hyperparameters like that to optimize test error. We could also enlarge our search, and try to find the _model_ that mimizes test error. Let's try, say, a **support vector regression** model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px3NhyF8YHus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "claim_size_model = SVR(kernel='sigmoid', gamma='scale')\n",
        "claim_size_model.fit(train_features, train_responses)\n",
        "\n",
        "train_mae = np.mean(np.abs(claim_size_model.predict(train_features) - train_responses))\n",
        "test_mae = np.mean(np.abs(claim_size_model.predict(test_features) - test_responses))\n",
        "print(f\"My predictions are off by ±{train_mae:.0f}$ on the train set on average, and by ±{test_mae:.0f}$ on the test set.\")\n",
        "\n",
        "print(f\"The average claim is {average_claim:.0f}$, so the relative test error is {100*test_mae/average_claim:.2f}%.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txOkoLIOYxUx",
        "colab_type": "text"
      },
      "source": [
        "And we can keep playing this game for a while. There is a catch, however.\n",
        "\n",
        "You might not have noticed, but we didn't actually use all the data for training and testing. There is still some left. How about, just for fun, checking the performance of our best model on the data we _didn't_ use either for training or testing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb1v9HXLZmcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "other_set = shuffled_policies[1000:]\n",
        "\n",
        "other_features = data['features_with_claims'][other_set, :]\n",
        "other_responses = data['nonzero_claims'][other_set]\n",
        "\n",
        "other_mae = np.mean(np.abs(claim_size_model.predict(other_features) - other_responses))\n",
        "print(f\"My predictions are off by ±{other_mae:.0f}$ on the other set on average.\")\n",
        "\n",
        "print(f\"The average claim is {average_claim:.0f}$, so the relative test error is {100*other_mae/average_claim:.2f}%.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeTVOARIeqcz",
        "colab_type": "text"
      },
      "source": [
        "Wait! It's not as good! What happened?\n",
        "\n",
        "Well, as we keep optimizing over the test set, a bias crept in. We evaluated several models and threw away the bad ones, keeping only the best one (on the test set). In some sense, information about the test set was eventually used to train the model (albeit in a subtle way). This gave us an over-optimistic picture of how much we can expect to predict on new data.\n",
        "\n",
        "This being said, we still improved somewhat compared to the initial test performance, so it wasn't all useless. It's very possible that the ultimate model we ended up choosing (the SVR) would really be the best model on average on new data. But we shouldn't trust the (over-optimized) test error as an actual picture of the performance to expect on new data.\n",
        "\n",
        "In practice, people do what we just did, they're just clear about it. Namely, it is typical to divide the training set into a **train/validation/test** split:\n",
        "\n",
        "- The **train** set is as before used for optimization of parameters.\n",
        "- The **validation** set is what we called the test set: a set of data for optimization of hyperparameters and model selection.\n",
        "- The **test** set was like our \"other\" set, which should be seen only once at the end and **never** used to influence any modelling decision (otherwise bias will creep in again). It's only there to give a final picture.\n",
        "\n",
        "A 70%/15%/15% split (or something around that) is typical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjLp7-EPU2Ry",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1.5 Exercise\n",
        "\n",
        "To practice a bit, let's try to fit a random forest classifier to predict nonzero claims. You should split your data into a 7000 train examples, 1500 validation examples and 1500 test examples. How high can you get in validation accuracy? What's your final test accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwnxiFAtVt2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "nonzero_claim_model = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "features = data['features']\n",
        "responses = data['has_claim']\n",
        "\n",
        "# 1. Split your data randomly\n",
        "# ...\n",
        "\n",
        "# 2. Train your model - what validation accuracy do you get?\n",
        "# ...\n",
        "\n",
        "# 3. Try playing with the model hyperparameters. How high can you get in validation accuracy?\n",
        "# ...\n",
        "\n",
        "# 4. Once you are done, check your test accuracy. How high is it?\n",
        "# N.B. Don't cheat, only look at it at the very end! You only have one shot!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZrqUX2CvHuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Possible solution\n",
        "\n",
        "# 1.\n",
        "shuffled_policies = np.random.permutation(np.arange(10000))\n",
        "\n",
        "train_set = shuffled_policies[:7000]\n",
        "validation_set = shuffled_policies[7000:8500]\n",
        "test_set = shuffled_policies[8500:]\n",
        "\n",
        "train_features = data['features'][train_set, :]\n",
        "train_responses = data['has_claim'][train_set]\n",
        "\n",
        "validation_features = data['features'][validation_set, :]\n",
        "validation_responses = data['has_claim'][validation_set]\n",
        "\n",
        "test_features = data['features'][test_set, :]\n",
        "test_responses = data['has_claim'][test_set]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6vLVo-mvz8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2.\n",
        "nonzero_claim_model.fit(train_features, train_responses)\n",
        "\n",
        "train_accuracy = nonzero_claim_model.score(train_features, train_responses)\n",
        "validation_accuracy = nonzero_claim_model.score(validation_features, validation_responses)\n",
        "print(f\"Train accuracy: {100*train_accuracy:.2f}%, validation accuracy: {100*validation_accuracy:.2f}%.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbjGIcj3wQW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3.\n",
        "# Here you can play for a bit - I'm just going to try two variants\n",
        "nonzero_claim_model = RandomForestClassifier(n_estimators=100, min_samples_leaf=10)\n",
        "nonzero_claim_model.fit(train_features, train_responses)\n",
        "\n",
        "train_accuracy = nonzero_claim_model.score(train_features, train_responses)\n",
        "validation_accuracy = nonzero_claim_model.score(validation_features, validation_responses)\n",
        "print(f\"Variant 1 | Train accuracy: {100*train_accuracy:.2f}%, validation accuracy: {100*validation_accuracy:.2f}%.\")\n",
        "\n",
        "nonzero_claim_model = RandomForestClassifier(n_estimators=100, min_samples_split=5)\n",
        "nonzero_claim_model.fit(train_features, train_responses)\n",
        "\n",
        "train_accuracy = nonzero_claim_model.score(train_features, train_responses)\n",
        "validation_accuracy = nonzero_claim_model.score(validation_features, validation_responses)\n",
        "print(f\"Variant 2 | Train accuracy: {100*train_accuracy:.2f}%, validation accuracy: {100*validation_accuracy:.2f}%.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqGGEzhZwkS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4.\n",
        "# I take the model with best validation accuracy\n",
        "nonzero_claim_model = RandomForestClassifier(n_estimators=100, min_samples_leaf=10)\n",
        "nonzero_claim_model.fit(train_features, train_responses)\n",
        "\n",
        "# and I evaluate the test accuracy\n",
        "test_accuracy = nonzero_claim_model.score(test_features, test_responses)\n",
        "print(f\"Test accuracy: {100*train_accuracy:.2f}%.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Lf_hjvurNk",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Unsupervised learning\n",
        "\n",
        "To complete our overview of machine learning, let's consider another kind of task that might be performed: some unsupervised learning.\n",
        "\n",
        "In unsupervised learning, concepts on \"training\" and \"test\" errors are much more subtle, and there is disagreement as to whether the concept of overfitting even makes sense. So we won't be talking about that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DElMZCH-k5Qd",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2.1 Principal components analysis\n",
        "\n",
        "Our first task we be to reduce the dimension of the data a bit to better vizualize it. We will fit a principal components analysis model on our continuous features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQuUrXGClKno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2, whiten=True)\n",
        "pca.fit(data['features'][:, :15])\n",
        "\n",
        "data['reduced_features'] = pca.transform(data['features'][:, :15])\n",
        "\n",
        "print(f\"Shape before: {data['features'][:, :15].shape}, shape after PCA: {data['reduced_features'].shape}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d_K-5G-li13",
        "colab_type": "text"
      },
      "source": [
        "Our data has been distilled to two abstract \"features\". How about vizualizing the results?\n",
        "\n",
        "We will use a library called **matplotlib** for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy-10UselcXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x, y = np.split(data['reduced_features'], indices_or_sections=2, axis=1)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkSqL7nTmB08",
        "colab_type": "text"
      },
      "source": [
        "Very interesting! There seems to be two \"groups\" of policies: those whose second PCA component is around zero, and those whose second PCA component is around 4ish.\n",
        "\n",
        "As a next task, let's use **k-means** to try to recuperate those groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5Qc7-rOmBHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "cluster_assignments = kmeans.fit_predict(data['reduced_features'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCNRcPh9m1sD",
        "colab_type": "text"
      },
      "source": [
        "Great! Let's now try to color the plot points by cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sSQ0zknri69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(np.squeeze(x), np.squeeze(y), c=cluster_assignments)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQhh_NZDrnBA",
        "colab_type": "text"
      },
      "source": [
        "At this point a natural next step would be to investigate more in depth what the points in each cluster have in common, to assign them potential meaning. Thus, unsupervised learning is often a prelimnary step in exploratory of analysis of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAJsEWoUuvGF",
        "colab_type": "text"
      },
      "source": [
        "##  3. Intro to Pytorch\n",
        "\n",
        "We have seen we can do tensor algebra with `numpy`. However, for deep learning, we need more than that! We need \n",
        "1. automatic differentiation\n",
        "2. GPU support\n",
        "\n",
        "**`pytorch`** is a library that supports that, with additional bells and whisles. There has been attempts to use it for other purposes as well (e.g. as a backend for graphical model inference), but by and large it is usually seen as a deep learning library.\n",
        "\n",
        "\n",
        "In `pytorch`, tensor algebra operations leave a trace in a computational graph that keeps tracks of relationship between tensors. Thus, when running an operation such as `C = A @ B`, `pytorch` will remember that `C` is the result of a tensor multipliation between `A` and `B`. This is useful because you can also ask `pytorch` to compute the gradient of a one dimensional tensor with respect to any original tensor using the `backward()` method. Thus, for example, running `C.sum().backward()` will compute the gradients of the sum of all components of `C` with respect to `A` and `B`. The resulting gradients will then be accessible as `A.grad` and `B.grad`.  This is a key ingredient in gradient-based parameter optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vnAYOei0LDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "A = torch.rand(5, 5)\n",
        "B = torch.rand(5, 5)\n",
        "\n",
        "A.requires_grad = True\n",
        "B.requires_grad = True\n",
        "\n",
        "C = A @ B\n",
        "C.sum().backward()\n",
        "print(f\"Gradient of A:\\n{A.grad}\")\n",
        "print(\"\")\n",
        "print(f\"Gradient of B:\\n{B.grad}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bQ_bJJs0H3f",
        "colab_type": "text"
      },
      "source": [
        "Moreover, `pytorch` natively supports doing tensor algebra on a GPU, through the CUBLAS/CULAPACK/CUDNN libraries. For example,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUxbOmKJwkB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "A = torch.rand(5, 5)\n",
        "B = torch.rand(5, 5)\n",
        "\n",
        "A_on_GPU, B_on_GPU = A.cuda(), B.cuda()  # Move the tensors to the GPU\n",
        "\n",
        "C_on_GPU = A_on_GPU @ B_on_GPU  # This matrix multiplication is done on the GPU!\n",
        "C_on_GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apw_WJsG3DUh",
        "colab_type": "text"
      },
      "source": [
        "For tensor algebra on large tensors, operations on a GPU are usually  _much_ faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmAycRCK3B8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "\n",
        "A = torch.rand(1000, 1000)\n",
        "B = torch.rand(1000, 1000)\n",
        "\n",
        "# 1. CPU\n",
        "start_time = time.time()\n",
        "A @ B\n",
        "end_time = time.time()\n",
        "CPU_total_time = end_time - start_time\n",
        "print(f\"CPU total time: {1e3*CPU_total_time:.2f} milliseconds\")\n",
        "\n",
        "# 2. GPU\n",
        "A_on_GPU, B_on_GPU = A.cuda(), B.cuda()\n",
        "\n",
        "start_time = time.time()\n",
        "A_on_GPU @ B_on_GPU\n",
        "end_time = time.time() \n",
        "GPU_total_time = end_time - start_time\n",
        "print(f\"GPU total time: {1e3*GPU_total_time:.2f} milliseconds\")\n",
        "\n",
        "print(f\"\\nGPU is {CPU_total_time/GPU_total_time:.2f} times faster in this case\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwmspaXe5FUW",
        "colab_type": "text"
      },
      "source": [
        "A thing to keep in mind, however, is that moving data between the CPU and the GPU has a non-negligible computational cost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXjcAmyV5Rfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "A.cuda(), B.cuda()\n",
        "end_time = time.time()\n",
        "CPU_to_GPU_copy_cost = end_time - start_time\n",
        "print(f\"CPU to GPU copy cost: {1e3*CPU_to_GPU_copy_cost:.2f} milliseconds\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELPqxG8B59EL",
        "colab_type": "text"
      },
      "source": [
        "Thus it is usually beneficial to do as many operations as possible on the GPU, to offset the initial copy cost.\n",
        "\n",
        "In practice, the actual time improvement will depend on many factors, including how much time is spent doing linear algebra. From anectodal evidence, I usually found that training a deep neural net on a GPU lead to a **10x speed improvement** vs. training it on a CPU. Your mileage may vary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUvL_QE7VHf-",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 A multilayer perceptron\n",
        "\n",
        "As a concrete example of what can be done with `pytorch`, we will fit a multilayer perceptron on the claim prediction task.\n",
        "\n",
        "We first start by splitting our data again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQtJMxwVbBxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffled_policies = np.random.permutation(np.arange(10000))\n",
        "\n",
        "train_set = shuffled_policies[:7000]\n",
        "validation_set = shuffled_policies[7000:8500]\n",
        "test_set = shuffled_policies[8500:]\n",
        "\n",
        "train_features = data['features'][train_set, :].astype(np.float32)\n",
        "train_responses = data['has_claim'][train_set].astype(np.int64)\n",
        "validation_features = data['features'][validation_set, :].astype(np.float32)\n",
        "validation_responses = data['has_claim'][validation_set].astype(np.int64)\n",
        "test_features = data['features'][test_set, :].astype(np.float32)\n",
        "test_responses = data['has_claim'][test_set].astype(np.int64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4uRyTOpfLf-",
        "colab_type": "text"
      },
      "source": [
        "These are numpy arrays living on the CPU, so the first thing we need to do is to convert them to `pytorch` tensors and move them to the GPU. (The dataset is small enough to all fit in GPU memory.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsGQu-O7fKU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = torch.from_numpy(train_features).cuda()\n",
        "train_responses = torch.from_numpy(train_responses).cuda()\n",
        "validation_features = torch.from_numpy(validation_features).cuda()\n",
        "validation_responses = torch.from_numpy(validation_responses).cuda()\n",
        "test_features = torch.from_numpy(test_features).cuda()\n",
        "test_responses = torch.from_numpy(test_responses).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPI47seKOT74",
        "colab_type": "text"
      },
      "source": [
        "Nowadays, all neural networks are trained by gradient descent with from randomly chosen initial parameters. The distributions from which these initial parameters are chosen assume (or make most sense) if the inputs are normalized between -1 and 1. Therefore, to provide reasonable training, it is usually always recommended to normalize inputs to neural networks, as we will do here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMybyjZrO8cB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "center = train_features.mean(dim=0).unsqueeze(0)\n",
        "scale = train_features.std(dim=0).clamp(min=1).unsqueeze(0)\n",
        "train_features = (train_features - center) / scale\n",
        "validation_features = (validation_features - center) / scale\n",
        "test_features = (test_features - center) / scale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hoa-UnfcNnO",
        "colab_type": "text"
      },
      "source": [
        "Recall, as you saw thing morning with Marzieh, that MLPs are the simplest kind of neural network, with a single hidden layer. We will use, say, 32 units everywhere and ReLU activations.\n",
        "\n",
        "We could implement the MLP by hand, defining weight and bias matrices,  but `pytorch` comes with implementations of most types of layers found in the literature. Using that language, for example, we can define an MLP as follows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvP0W7SscKRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_input_features = train_features.shape[-1]\n",
        "\n",
        "mlp = torch.nn.Sequential(\n",
        "        torch.nn.Linear(nb_input_features, 32),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(32, 2),\n",
        "        torch.nn.Softmax(dim=-1)).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XGWsMP7FWEh",
        "colab_type": "text"
      },
      "source": [
        "We can now feed the entirety of our training dataset and it will produce predictions for every 7000 samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkmg3ScYdK68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = mlp(train_features)\n",
        "print(predictions.shape)\n",
        "print(predictions[:5, :])\n",
        "\n",
        "accuracy = (train_responses == predictions.argmax(dim=-1)).float().mean()\n",
        "print(f\"\\nAccuracy: {100*accuracy:.1f}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IanDqPo5G5Vt",
        "colab_type": "text"
      },
      "source": [
        "We will fit our MLP by gradient descent, a standard approach. This corresponds to taking steps in the direction of the gradient of the loss with respect to the parameters until the gradient becomes (close to) zero, in which case the process stops. In our case, since we have a classification problem, we will use the \"cross-entropy loss\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv3FyXylJbWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy_loss = -(predictions + 1e-5).log().gather(1, train_responses.unsqueeze(-1)).mean()\n",
        "print(cross_entropy_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89mkAD6MK0A5",
        "colab_type": "text"
      },
      "source": [
        "We can obtain the gradients by calling `backward` on `cross_entropy_loss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf-FD4E9K8tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp.zero_grad()\n",
        "cross_entropy_loss.backward()\n",
        "\n",
        "for name, parameter in mlp.named_parameters():\n",
        "  print(f\"Gradient of parameter {name}:\")\n",
        "  print(parameter.grad)\n",
        "  print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id84IJeFLxkJ",
        "colab_type": "text"
      },
      "source": [
        "Now we can apply our gradient update from those gradients:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnOw2DDULxCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for parameter in mlp.parameters():\n",
        "  parameter.data -= 1e-2 * parameter.grad.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B_ZGcfgSI3B",
        "colab_type": "text"
      },
      "source": [
        "Tadam! We just did our first gradient update! We should now have slightly better training performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfO5YonMSLtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = mlp(train_features)\n",
        "print(predictions.shape)\n",
        "print(predictions[:5, :])\n",
        "\n",
        "accuracy = (train_responses == predictions.argmax(dim=-1)).float().mean()\n",
        "print(f\"\\nAccuracy: {100*accuracy:.1f}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d2sjDXqMF8n",
        "colab_type": "text"
      },
      "source": [
        "If we repeat this over and over again, our parameters will slowly move towards values that minimize this cross-entropy loss (hence increase classification accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRWhaGZYMFhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for update in range(25):\n",
        "  predictions = mlp(train_features)\n",
        "  cross_entropy_loss = -(predictions + 1e-5).log().gather(1, train_responses.unsqueeze(-1)).mean()\n",
        "  mlp.zero_grad()\n",
        "  cross_entropy_loss.backward()\n",
        "  for parameter in mlp.parameters():\n",
        "    parameter.data -= 1e-2 * parameter.grad.data\n",
        "  accuracy = (train_responses == predictions.argmax(dim=-1)).float().mean()\n",
        "  print(f\"Cross entropy loss: {cross_entropy_loss:.4f}, accuracy: {100*accuracy:.1f}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWdD-O45TCNz",
        "colab_type": "text"
      },
      "source": [
        "Neural networks can overfit, so it is usually a good idea to monitor the validation error as we update and stop when further updates worsen the validation error (\"early stopping\").\n",
        "\n",
        "For MLPs this is less problematic since they tend to lack capacity, but it's good to doublecheck nonetheless."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2Z-9c8BTMQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = torch.nn.Sequential(\n",
        "        torch.nn.Linear(nb_input_features, 32),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(32, 2),\n",
        "        torch.nn.Softmax(dim=-1)).cuda()\n",
        "\n",
        "for update in range(50):\n",
        "  train_predictions = mlp(train_features)\n",
        "  train_cross_entropy_loss = -(train_predictions + 1e-5).log().gather(1, train_responses.unsqueeze(-1)).mean()\n",
        "  mlp.zero_grad()\n",
        "  train_cross_entropy_loss.backward()\n",
        "  for parameter in mlp.parameters():\n",
        "    parameter.data -= 1e-2 * parameter.grad.data\n",
        "  train_accuracy = (train_responses == train_predictions.argmax(dim=-1)).float().mean()\n",
        "  \n",
        "  validation_predictions = mlp(validation_features)\n",
        "  validation_cross_entropy_loss = -(validation_predictions + 1e-5).log().gather(1, validation_responses.unsqueeze(-1)).mean()\n",
        "  validation_accuracy = (validation_responses == validation_predictions.argmax(dim=-1)).float().mean()\n",
        "  print(f\"Update {update}\")\n",
        "  print(f\"  Training   cross entropy loss: {train_cross_entropy_loss:.4f}, accuracy: {100*train_accuracy:.1f}%\")\n",
        "  print(f\"  Validation cross entropy loss: {validation_cross_entropy_loss:.4f}, accuracy: {100*validation_accuracy:.1f}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avGMNXW1U9pm",
        "colab_type": "text"
      },
      "source": [
        "And of course we can check the test performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp6C9hNAVbPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predictions = mlp(test_features)\n",
        "test_cross_entropy_loss = -(test_predictions + 1e-5).log().gather(1, test_responses.unsqueeze(-1)).mean()\n",
        "test_accuracy = (test_responses == test_predictions.argmax(dim=-1)).float().mean()\n",
        "print(f\"Test cross entropy loss: {test_cross_entropy_loss:.4f}, accuracy: {100*test_accuracy:.1f}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCiOqlmeVoSI",
        "colab_type": "text"
      },
      "source": [
        "That's not bad at all! (Compare with previous GP and RF test accuracies.) Of course we could do some hyperparameter tuning here on the validation error, as usual."
      ]
    }
  ]
}