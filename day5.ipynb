{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hotbread213/createClass/blob/master/day5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4RfKm9UApCB",
        "colab_type": "text"
      },
      "source": [
        "# Fin-ML/IVADO Finance and Insurance Workshop\n",
        "## Reinforcement learning\n",
        "Day 5 afternoon tutorial, March 8th 2019\n",
        "\n",
        "1. A simple stock market problem\n",
        "2. Q-learning\n",
        "3. Q-learning with linear approximation\n",
        "4. Exercice\n",
        "5. Q-learning with linear approximation and experience replay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEBcy9PRDUZ2",
        "colab_type": "text"
      },
      "source": [
        "## 1. A simple stock market problem\n",
        "\n",
        "There is a variety of finance problems that can be naturally formulated as control problems (finding an optimal policy for an MDP). Today we'll see an artificial example and look at different reinforcement learning algorithms to solve it.\n",
        "\n",
        "We will first download the code for the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfQ8KNNZtklp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O /content/environment.py https://www.dropbox.com/s/bfxh3nb4b1iiey7/environment.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KUp0mFyqW2f",
        "colab_type": "text"
      },
      "source": [
        "The concept of the game is as follows. You are a broker charged with selling 10 shares of a stock. The bid price follows a roughly sinusoidal pattern. At each timestep, you are given info about the state of the limit order book, and you have 20 timesteps to sell all your shares.\n",
        "\n",
        "Notes:\n",
        "- If at the end of the 20 timesteps you have shares left they will be sold at the current price. \n",
        "- If you try to sell more shares than you have left, your order will be corrected to your inventory.\n",
        "\n",
        "Sounds good? Let's see this in action.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLE_UXxiHSb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from environment import Environment\n",
        "\n",
        "env = Environment()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHkVY3abhbtO",
        "colab_type": "text"
      },
      "source": [
        "Let's start a new episode. The environment returns the initial state $s_0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yeq7_DFTu_Jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_state = env.new_episode()\n",
        "return_ = 0\n",
        "print(f\"The initial state is\\n\\n    s_0 = {initial_state}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UtF2vrdhkW4",
        "colab_type": "text"
      },
      "source": [
        "The state $s_t$ at time $t$ is composed of the following features, on which we can base ourselves to take decisions.\n",
        "\n",
        "*   __bid_price__: The bid price of the asset at time $t$.\n",
        "*   __bid_volume__: The volume of bids at time $t$.\n",
        "*  __spread__: The bid-ask spead at time $t$.\n",
        "*   __ask_volume__: The volume of asks at time $t$.\n",
        "*  __n_trades__: The number of trades between $t-1$ and $t$.\n",
        "* __inventory__: The amount of shares that are left to be sold at time $t$.\n",
        "\n",
        "In your opinion, what should we do now?\n",
        "Let's assume for simplicity we can only sell 0-2 shares at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csX_JgiXLrhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ACTION = 2\n",
        "\n",
        "state, reward, done = env.step(ACTION)\n",
        "return_ += reward\n",
        "\n",
        "print(f\"We made a reward of {reward:.2f}$.\")\n",
        "if done:\n",
        "    print(f\"The episode is over.\")\n",
        "    print(f\"We made a total return of {return_:.2f}$ over the episode.\")\n",
        "else:\n",
        "    print(f\"The total return so far is {return_:.2f}$.\\n\")\n",
        "    print(f\"The new state is\\n\\n     s_{env.timestep} = {state}.\\n\")\n",
        "    print(f\"What should we do now?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMSrmjQ7IkFs",
        "colab_type": "text"
      },
      "source": [
        "Our goal is to teach an agent to maximize such returns. Reinforcement learning can help.\n",
        "\n",
        "But first, it might be interesting to see what can of performance we can get from an agent that takes random decisions. What is the average return of such an agent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmACXAcwJu2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "state = env.new_episode()\n",
        "done = False\n",
        "return_ = 0\n",
        "while not done:\n",
        "  action = int(np.random.choice(3))\n",
        "\n",
        "  state, reward, done = env.step(action)\n",
        "  return_ += reward\n",
        "print(f\"I made {return_:.2f}$.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDFVoceXM0FI",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the return is stochastic. Therefore, it would be more interesting to look at the distribution of returns of such an agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSzU1P7nM9i8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "returns = []\n",
        "for _ in range(10000):\n",
        "  state = env.new_episode()\n",
        "  done = False\n",
        "  return_ = 0\n",
        "  while not done:\n",
        "    action = int(np.random.choice(np.arange(3)))\n",
        "\n",
        "    state, reward, done = env.step(action)\n",
        "    return_ += reward\n",
        "  returns.append(return_)\n",
        "\n",
        "print(f\"My average return was {np.mean(returns):.2f}$ with standard deviation ±{np.std(returns):.2f}$.\")\n",
        "plt.hist(returns)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_vg6pEUN_wZ",
        "colab_type": "text"
      },
      "source": [
        "Okay! Well, this can serve an a baseline - if we can't make more money than that on average, we are not very smart!\n",
        "\n",
        "Let's see if reinforcement learning can do better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "777YqCyMOeTX",
        "colab_type": "text"
      },
      "source": [
        "## 2. Q-learning\n",
        "\n",
        "As first RL algorithm, we will implement (tabular) Q-learning.\n",
        "Tabular means there will be no statistical inference between states: every state will be treated distinctly, all equally dissimilar.\n",
        "\n",
        "Our first obstacle is that the states are a mix of continuous and integer-valued features: we need to discretize them to apply tabular methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r320jfjtOUFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_bins = [[0, 8, 20], \n",
        "              [0, 8, 20], \n",
        "              [0, 8, 20], \n",
        "              [0, 8, 20], \n",
        "              [0, 8, 20],\n",
        "              [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]\n",
        "\n",
        "def discretize(state):\n",
        "  discretized_state = []\n",
        "  for feature, bins in zip(state, state_bins):\n",
        "    discretized_state.append(int(np.digitize(feature, bins))-1)\n",
        "  return tuple(discretized_state)\n",
        "\n",
        "print(f\"Raw state:         {state}\")\n",
        "print(f\"Discretized state: {discretize(state)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4-NMM9fWdx_",
        "colab_type": "text"
      },
      "source": [
        "Here I used a simple binning system, where every feature is binary. This loses a lot of information but is a simple way to get started.\n",
        "\n",
        "We are now ready to do Q-learning. We will attempt to estimate $Q_*(s, a)$, the expected return from taking action $a$ in state $s$ and hereafter behaving optimally. How can we compute such a function?\n",
        "\n",
        "Well, we don't know much about $Q_*$, but we know it must satisfy **Bellman's equation**\n",
        "\n",
        "$$Q_*(s, a) = \\text{E}\\bigg[R+\\max_{a'}Q_*(S', a')\\;\\bigg\\vert\\;S=s, A=a\\bigg]$$\n",
        "\n",
        "with the conditions:\n",
        "$$Q_*(s, \\cdot) \\equiv 0\\qquad\\text{for any terminal }s\\in S.$$\n",
        "\n",
        "This suggests an algorithm (Q-learning) as follows. First, we will create our Q-function table and initialize it randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SStURAArSWtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q = np.random.rand(*(len(bins)-1 for bins in state_bins), 3)\n",
        "print(f\"Shape of Q: {Q.shape}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpPUpLiFTlWx",
        "colab_type": "text"
      },
      "source": [
        "Next, we will simulate runs of an agent that behaves as follows when in a state $s$:\n",
        "- 90% of the time, it will pick the action $a^*=\\arg\\max_a Q(s, a)$;\n",
        "- 10% of the time, it will pick a random action, $a\\sim \\text{Uniform}\\{0,...,10\\}$,\n",
        "\n",
        "For example,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo6vwM5obUPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.new_episode()\n",
        "explore = np.random.rand() < 0.1\n",
        "if explore:\n",
        "  action = np.random.choice(np.arange(3))\n",
        "else:\n",
        "  action_values = Q[discretize(state) + (...,)]\n",
        "  action = action_values.argmax()\n",
        "print(f\"In state   {state},\\nI chose to sell {action} items\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTzW6Qumggo8",
        "colab_type": "text"
      },
      "source": [
        "We act accordingly..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO54Ht6Qgm2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next_state, reward, done = env.step(action)\n",
        "print(f\"I got {reward:.2f}$ in immediate reward,\\nand the new state is {next_state}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hrwsTwvhCTL",
        "colab_type": "text"
      },
      "source": [
        "... and make a soft, Bellman-style update to our Q-function estimate,\n",
        "\n",
        "$$Q(s, a) = 0.5\\cdot Q(s,a) + 0.5\\cdot\\big[r+\\max_{a'}Q(s', a')\\big]$$\n",
        "\n",
        "that is\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co2MWRsThSJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q[discretize(state) + (action,)] = 0.5 * Q[discretize(state) + (action,)] \\\n",
        "                                 + 0.5 * (reward + Q[discretize(next_state) + (...,)].max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-11-6KSPooYA",
        "colab_type": "text"
      },
      "source": [
        "We can continue doing this until we reach the end of the episode,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAQqlmphonoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "while True:\n",
        "  state = next_state\n",
        "  explore = np.random.rand() < 0.1\n",
        "  if explore:\n",
        "    action = np.random.choice(np.arange(0, 3))\n",
        "  else:\n",
        "    action_values = Q[discretize(state) + (...,)]\n",
        "    action = action_values.argmax()\n",
        "  print(f\"I chose to sell {action} items\")\n",
        "\n",
        "  next_state, reward, done = env.step(action)\n",
        "  print(f\"I got a reward of {reward:.2f}$\")\n",
        "  if done:\n",
        "    print(\"End of episode\")\n",
        "    break\n",
        "\n",
        "  Q[discretize(state) + (action,)] = 0.5 * Q[discretize(state) + (action,)] \\\n",
        "                                   + 0.5 * (reward + Q[discretize(next_state) + (...,)].max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2x-o5EstO8f",
        "colab_type": "text"
      },
      "source": [
        "And now that we reached a terminal state, we should do a hard update\n",
        "\n",
        "$$Q(s, a) = r$$\n",
        "\n",
        "because we don't know much about the optimal Q, but we do know it should be zero for terminal states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPQtuuVGqDsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q[discretize(state) + (action,)] = reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O98nvVfDug0p",
        "colab_type": "text"
      },
      "source": [
        "In summary, this is what a training loop over a single episode looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWgVC9xPvBrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.new_episode()\n",
        "return_ = 0\n",
        "while True:\n",
        "  explore = np.random.rand() < 0.1\n",
        "  if explore:\n",
        "    action = np.random.choice(np.arange(3))\n",
        "  else:\n",
        "    action_values = Q[discretize(state) + (...,)]\n",
        "    action = action_values.argmax()\n",
        "  print(f\"I chose to sell {action} items\")\n",
        "\n",
        "  next_state, reward, done = env.step(action)\n",
        "  print(f\"I got a reward of {reward:.2f}$\")\n",
        "  return_ += reward\n",
        "  if not done:\n",
        "    Q[discretize(state) + (action,)] = 0.5 * Q[discretize(state) + (action,)] \\\n",
        "                                     + 0.5 * (reward + Q[discretize(next_state) + (...,)].max())\n",
        "    state = next_state\n",
        "  else:\n",
        "    Q[discretize(state) + (action,)] = reward\n",
        "    print(f\"End of episode, my return was {return_:.2f}\")\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I825lZwWwO9r",
        "colab_type": "text"
      },
      "source": [
        "Progressively, the $Q$-function will magically start to converge towards the optimal $Q_*$.\n",
        "\n",
        "(At least, optimal for a given discretization scheme.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rJFIQbOwuPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q = np.random.rand(*(len(bins)-1 for bins in state_bins), 3)\n",
        "for episode in range(5000):\n",
        "  state = env.new_episode()\n",
        "  return_ = 0\n",
        "  while True:\n",
        "    explore = np.random.rand() < 0.1\n",
        "    if explore:\n",
        "      action = np.random.choice(np.arange(3))\n",
        "    else:\n",
        "      action_values = Q[discretize(state) + (...,)]\n",
        "      action = action_values.argmax()\n",
        "\n",
        "    next_state, reward, done = env.step(action)\n",
        "    return_ += reward\n",
        "    if not done:\n",
        "      Q[discretize(state) + (action,)] = 0.5 * Q[discretize(state) + (action,)] \\\n",
        "                                       + 0.5 * (reward + Q[discretize(next_state) + (...,)].max())\n",
        "      state = next_state\n",
        "    else:\n",
        "      Q[discretize(state) + (action,)] = 0.5 * Q[discretize(state) + (action,)] \\\n",
        "                                       + 0.5 * reward\n",
        "      break\n",
        "  if episode % 1000 == 0:\n",
        "    print(f\"Episode {episode}, my return was {return_:.2f}\")\n",
        "print(\"Done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh7XYXpix-D-",
        "colab_type": "text"
      },
      "source": [
        "We can now check if our greedy agent (the one that always follows $a^*=\\arg\\max_a Q(s, a)$) does better than a random agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oZpqudTx0Z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "returns = []\n",
        "for _ in range(10000):\n",
        "  state = env.new_episode()\n",
        "  done = False\n",
        "  return_ = 0\n",
        "  while not done:\n",
        "    action_values = Q[discretize(state) + (...,)]\n",
        "    action = action_values.argmax()\n",
        "\n",
        "    state, reward, done = env.step(action)\n",
        "    return_ += reward\n",
        "  returns.append(return_)\n",
        "\n",
        "print(f\"My average return was {np.mean(returns):.2f}$ with standard deviation ±{np.std(returns):.2f}$.\")\n",
        "plt.hist(returns)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9mnrBjhIomo",
        "colab_type": "text"
      },
      "source": [
        "So it looks like we got some improvements - how about plotting return by episode? We could, say, train our Q-learning 10000 random agent like before, and every training episode we check the average return of our agent vs a random agent over 5 seeds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1BQlhrAJOKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_episode(agent):\n",
        "  \"\"\" Runs an episode for an agent, returns the return.\"\"\"\n",
        "  state = env.new_episode()\n",
        "  done = False\n",
        "  return_ = 0\n",
        "  while not done:\n",
        "    if agent == \"RL\":\n",
        "      action_values = Q[discretize(state) + (...,)]\n",
        "      action = action_values.argmax()\n",
        "    elif agent == \"random\":\n",
        "      action = int(np.random.choice(np.arange(3)))\n",
        "\n",
        "    state, reward, done = env.step(action)\n",
        "    return_ += reward\n",
        "  return return_\n",
        "\n",
        "\n",
        "Q = np.random.rand(*(len(bins)-1 for bins in state_bins), 3)\n",
        "RL_returns = []\n",
        "random_returns = []\n",
        "for episode in range(5000):\n",
        "  # Training\n",
        "  state = env.new_episode()\n",
        "  while True:\n",
        "    explore = np.random.rand() < 0.1\n",
        "    if explore:\n",
        "      action = np.random.choice(np.arange(3))\n",
        "    else:\n",
        "      action_values = Q[discretize(state) + (...,)]\n",
        "      action = action_values.argmax()\n",
        "\n",
        "    next_state, reward, done = env.step(action)\n",
        "    if not done:\n",
        "      Q[discretize(state) + (action,)] = 0.5 * Q[discretize(state) + (action,)] \\\n",
        "                                       + 0.5 * (reward + Q[discretize(next_state) + (...,)].max())\n",
        "      state = next_state\n",
        "    else:\n",
        "      Q[discretize(state) + (action,)] = 0.5 * Q[discretize(state) + (action,)] \\\n",
        "                                       + 0.5 * reward\n",
        "      break\n",
        "  \n",
        "  # Evaluation\n",
        "  RL_return = np.mean([run_episode(\"RL\") for _ in range(5)])\n",
        "  RL_returns.append(RL_return)\n",
        "  random_return = np.mean([run_episode(\"random\") for _ in range(5)])\n",
        "  random_returns.append(random_return)\n",
        "  if episode % 1000 == 0:\n",
        "    print(f\"Episode {episode}, RL agent return was {RL_return:.2f}\"\n",
        "          f\", random agent return was {random_return:.2f}\")\n",
        "print(\"Done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PaG_LjPdVXk",
        "colab_type": "text"
      },
      "source": [
        "If we plot the returns this is what we find!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pgwHpUt5M3tK",
        "colab": {}
      },
      "source": [
        "smoothed_RL_returns = np.convolve(RL_returns, np.ones((100,))/100, mode='valid')\n",
        "smoothed_random_returns = np.convolve(random_returns, np.ones((100,))/100, mode='valid')\n",
        "\n",
        "plt.plot(np.arange(len(smoothed_RL_returns)), smoothed_RL_returns, '-', \n",
        "         np.arange(len(smoothed_random_returns)), smoothed_random_returns, '-')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC4PCeGT1HsV",
        "colab_type": "text"
      },
      "source": [
        "## 3. Q-learning with linear approximation\n",
        "\n",
        "To use Q-learning we had to discretize our state space. This is problematic because you lose information: in the worst case, if you get the bins completely wrong, all the states could be sent to the same bin and you might destroy all the information! Moreover, this adds additional hyperparameters and reinforcement learning algorithms, in general, tend to be very sentitive to hyperparameters.\n",
        "\n",
        "To address this, we will make use of function approximation and see what kind of improvements it might offer. In this case it will be a linear model, the easiest kind.\n",
        "\n",
        "Q-learning with linear function approximation works as follows. Whereas $Q(s,a)$ used to be a table, now for every $a$ $s\\mapsto Q(s, a)$ will be a linear regression model. We will explore the environment the usual way,\n",
        "- 90% of the time, pick the action $a^*=\\arg\\max_a Q(s, a)$,\n",
        "- 10% of the time, pick a random action, $a\\sim \\text{Uniform}\\{0,...,10\\}$,\n",
        "collection transitions $(s,a,r,s')$ along the way.\n",
        "\n",
        "What differs is how we update the Q-function after observing the transition. Usually, linear models are fitted offline, with all the data in one shot, but here we will keep observing new data and we do not want to forget older data. What is typical then is to train the linear model using an online scheme, such as stochastic gradient descent. Namely, after observing the transition $(s,a,r,s')$, we will update the linear model $Q(\\cdot, a)$ with a gradient descent step on a single $(x,y)$ pair\n",
        "\n",
        "$$\\big(s, r+\\max_{a'}Q(s',a')\\big).$$\n",
        "\n",
        "Let's see this in practice. Let's first create our linear models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlrydjLylSwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "Q = []\n",
        "for _ in range(3):\n",
        "  Q_a = SGDRegressor(alpha=0.05)\n",
        "  Q_a.partial_fit(np.zeros((1, 6)), [100])\n",
        "  Q.append(Q_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tt_dFo4lTWj",
        "colab_type": "text"
      },
      "source": [
        "I decided to initialize them by a single gradient descent step on the pair $(0, 100)$: that is, the zero state will be initialized to 100$.\n",
        "\n",
        "We can now start the usual way - create an environment and take an action, say, sell one share."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQOEr2fmfZI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.new_episode()\n",
        "action = 2\n",
        "next_state, reward, done = env.step(action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3XlADXQomhU",
        "colab_type": "text"
      },
      "source": [
        "After observing the transition, we can now update the relevant linear model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2k6efTSovwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next_values = np.array([Q[action].predict(next_state.reshape(1, -1)) \n",
        "                        for action in range(3)])\n",
        "Q[action].partial_fit(state.reshape(1, -1), [reward + next_values.max()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4qkfGFMo1lr",
        "colab_type": "text"
      },
      "source": [
        "And we can then continue traning over the episode, remembering that terminal states have zero value by construction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWTpsB5OpCBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.new_episode()\n",
        "return_ = 0\n",
        "while True:\n",
        "  explore = np.random.rand() < 0.1\n",
        "  if explore:\n",
        "    action = np.random.choice(np.arange(3))\n",
        "  else:\n",
        "    action_values = np.array([Q[action].predict(state.reshape(1, -1)) \n",
        "                              for action in range(3)])\n",
        "    action = action_values.argmax()\n",
        "  print(f\"I chose to sell {action} items\")\n",
        "\n",
        "  next_state, reward, done = env.step(action)\n",
        "  print(f\"I got a reward of {reward:.2f}$\")\n",
        "  return_ += reward\n",
        "  if not done:\n",
        "    next_values = np.array([Q[action].predict(next_state.reshape(1, -1)) \n",
        "                      for action in range(3)])\n",
        "    Q[action].partial_fit(state.reshape(1, -1), [reward + next_values.max()])\n",
        "    state = next_state\n",
        "  else:\n",
        "    Q[action].partial_fit(state.reshape(1, -1), [reward])\n",
        "    print(f\"End of episode, my return was {return_:.2f}\")\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVr-g1sZpavC",
        "colab_type": "text"
      },
      "source": [
        "Pretty good! And now, we can repeat this training over many episodes to train our agent.\n",
        "This is what we obtain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3ujzI2e3-Yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q = []\n",
        "for _ in range(3):\n",
        "  Q_a = SGDRegressor(alpha=0.05)\n",
        "  Q_a.partial_fit(np.zeros((1, 6)), [100])\n",
        "  Q.append(Q_a)\n",
        "\n",
        "\n",
        "def run_episode(agent):\n",
        "  \"\"\" Runs an episode for an agent, returns the return.\"\"\"\n",
        "  state = env.new_episode()\n",
        "  done = False\n",
        "  return_ = 0\n",
        "  while not done:\n",
        "    if agent == \"RL\":\n",
        "      action_values = np.array([Q[action].predict(state.reshape(1, -1)) \n",
        "                                for action in range(3)])\n",
        "      action = action_values.argmax()\n",
        "    elif agent == \"random\":\n",
        "      action = int(np.random.choice(np.arange(3)))\n",
        "\n",
        "    state, reward, done = env.step(action)\n",
        "    return_ += reward\n",
        "  return return_\n",
        "\n",
        "\n",
        "RL_returns = []\n",
        "random_returns = []\n",
        "for episode in range(1000):\n",
        "  # Training\n",
        "  state = env.new_episode()\n",
        "  while True:\n",
        "    explore = np.random.rand() < 0.1\n",
        "    if explore:\n",
        "      action = np.random.choice(np.arange(3))\n",
        "    else:\n",
        "      action_values = np.array([Q[action].predict(state.reshape(1, -1)) \n",
        "                                for action in range(3)])\n",
        "      action = action_values.argmax()\n",
        "\n",
        "    next_state, reward, done = env.step(action)\n",
        "    if not done:\n",
        "      next_values = np.array([Q[action].predict(next_state.reshape(1, -1)) \n",
        "                        for action in range(3)])\n",
        "      Q[action].partial_fit(state.reshape(1, -1), [reward + next_values.max()])\n",
        "      state = next_state\n",
        "    else:\n",
        "      Q[action].partial_fit(state.reshape(1, -1), [reward])\n",
        "      break\n",
        "  \n",
        "  # Evaluation\n",
        "  RL_return = np.mean([run_episode(\"RL\") for _ in range(5)])\n",
        "  RL_returns.append(RL_return)\n",
        "  random_return = np.mean([run_episode(\"random\") for _ in range(5)])\n",
        "  random_returns.append(random_return)\n",
        "  if episode % 200 == 0:\n",
        "    print(f\"Episode {episode}, RL agent return was {RL_return:.2f}\"\n",
        "          f\", random agent return was {random_return:.2f}\")\n",
        "print(\"Done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW-X8D2yqaNR",
        "colab_type": "text"
      },
      "source": [
        "And now we can plot the training curve..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tDNGZenCMvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smoothed_RL_returns = np.convolve(RL_returns, np.ones((100,))/100, mode='valid')\n",
        "smoothed_random_returns = np.convolve(random_returns, np.ones((100,))/100, mode='valid')\n",
        "\n",
        "plt.plot(np.arange(len(smoothed_RL_returns)), smoothed_RL_returns, '-', \n",
        "         np.arange(len(smoothed_random_returns)), smoothed_random_returns, '-')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eY7L4gEqhIo",
        "colab_type": "text"
      },
      "source": [
        "... and the histogram of returns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaOgAXdQV5v7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "returns = [run_episode(\"RL\") for _ in range(10000)]\n",
        "\n",
        "print(f\"My average return was {np.mean(returns):.2f}$ with standard deviation ±{np.std(returns):.2f}$.\")\n",
        "plt.hist(returns)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4KjlFVGqkYd",
        "colab_type": "text"
      },
      "source": [
        "What can we conclude? Well, first, it didn't really change the performance of the optimal policy. But, interestingly, the training was _much_ faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v-UdtQQ4031",
        "colab_type": "text"
      },
      "source": [
        "## 4. Exercice\n",
        "\n",
        "We will now try to turn this into practice. I made an exercice environment you can play with, which you can load like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnj1HXRL40E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from environment import ExerciceEnvironment\n",
        "  \n",
        "env = ExerciceEnvironment()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZlGoncbjfXM",
        "colab_type": "text"
      },
      "source": [
        "The environment is very similar to the one you just used before, but with small modifications:\n",
        "- There are now 7 state features;\n",
        "- The actions are now -1, 0 and 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEQcvTL86pUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.new_episode()\n",
        "print(f\"State: {state}\\n\")\n",
        "\n",
        "action = -1\n",
        "next_state, reward, done = env.step(action)\n",
        "print(f\"Next state: {next_state}\\nReward: {reward:.2f}$\\nDone: {done}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_xEg83k6p8g",
        "colab_type": "text"
      },
      "source": [
        "Now try to code a Q-learning algorithm. \n",
        "\n",
        "How good is your resulting agent? How fast did it learn?\n",
        "How far can you push it? Here are some hyperparameters to play with:\n",
        "- The SGD learning rate (default $\\alpha=0.05$).\n",
        "- The initial Q-function value (default 100 dollars)\n",
        "- The exploration rate (default $\\epsilon=0.1$)\n",
        "\n",
        "Good luck!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWdEiWY0wUEL",
        "colab_type": "text"
      },
      "source": [
        "## 5. Q-learning with linear approximation and experience replay\n",
        "\n",
        "Finally, we will cover experience replay, a trick that accelerates and stabilitizes Q-learning.\n",
        "A disadvantage of Q-learning, as shown before, is that samples are used once and then thrown away. This is somewhat wasteful.\n",
        "\n",
        "Experience replay is a simple trick to increase sample efficiency: we store transitions in a buffer, and when updating we batch some of these samples for an update.\n",
        "\n",
        "Let's see this in practice. First, let's create our environment again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQe9zCuDn5u0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Environment()\n",
        "\n",
        "Q = []\n",
        "for _ in range(3):\n",
        "  Q_a = SGDRegressor(alpha=0.05)\n",
        "  Q_a.partial_fit(np.zeros((1, 6)), [100])\n",
        "  Q.append(Q_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zrb1Uuk1M4k",
        "colab_type": "text"
      },
      "source": [
        "Let's now take an action, and learn from the transition like before, but! We also store it in a memory buffer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feH_GqhN17sO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "memory = []\n",
        "\n",
        "state = env.new_episode()\n",
        "action = int(np.random.choice(np.arange(3)))\n",
        "next_state, reward, done = env.step(action)\n",
        "memory.append((state, action, reward, next_state, float(done)))  # <------------\n",
        "\n",
        "next_values = np.array([Q[action].predict(next_state.reshape(1, -1)) \n",
        "                  for action in range(3)])\n",
        "Q[action].partial_fit(state.reshape(1, -1), [reward + next_values.max()])\n",
        "state = next_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4N9fMg23bCd",
        "colab_type": "text"
      },
      "source": [
        "Then at the next transition, we not only learn from the new transition but also the old one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFiajBlz2MTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action = int(np.random.choice(np.arange(3)))\n",
        "next_state, reward, done = env.step(action)\n",
        "memory.append((state, action, reward, next_state, float(done)))\n",
        "\n",
        "states, actions, rewards, next_states, dones = map(np.stack, zip(*memory))\n",
        "next_values = np.array([Q[action].predict(next_states) \n",
        "                  for action in range(3)])\n",
        "for action in actions:\n",
        "  Q[action].partial_fit(states[actions == action], \n",
        "                        (rewards + (1-dones) * next_values.max(axis=0))[actions == action])\n",
        "state = next_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0iwpaQ15E2Z",
        "colab_type": "text"
      },
      "source": [
        "Eventually the memory buffer will grow quite big, making updates slow and reducing the weight given to new observations. To counter-balance that, it is typical to:\n",
        "- Flush old transitions.\n",
        "- Sample from the buffer.\n",
        "\n",
        "Putting everything together in a loop, this is what we obtain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgROu2qJ6zy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "QER = []\n",
        "for _ in range(3):\n",
        "  Q_a = SGDRegressor(alpha=0.05)\n",
        "  Q_a.partial_fit(np.zeros((1, 6)), [100])\n",
        "  QER.append(Q_a)\n",
        "\n",
        "\n",
        "Q = []\n",
        "for _ in range(3):\n",
        "  Q_a = SGDRegressor(alpha=0.05)\n",
        "  Q_a.partial_fit(np.zeros((1, 6)), [100])\n",
        "  Q.append(Q_a)\n",
        "\n",
        " \n",
        " \n",
        "\n",
        "def run_episode(agent):\n",
        "  \"\"\" Runs an episode for an agent, returns the return.\"\"\"\n",
        "  state = env.new_episode()\n",
        "  done = False\n",
        "  return_ = 0\n",
        "  while not done:\n",
        "    if agent == \"RLER\":\n",
        "      action_values = np.array([QER[action].predict(state.reshape(1, -1)) \n",
        "                                for action in range(3)])\n",
        "      action = action_values.argmax()\n",
        "    if agent == \"RL\":\n",
        "      action_values = np.array([Q[action].predict(state.reshape(1, -1)) \n",
        "                                for action in range(3)])\n",
        "      action = action_values.argmax()\n",
        "    elif agent == \"random\":\n",
        "      action = int(np.random.choice(np.arange(3)))\n",
        "\n",
        "    state, reward, done = env.step(action)\n",
        "    return_ += reward\n",
        "  return return_\n",
        "\n",
        "\n",
        "RLER_returns = []\n",
        "RL_returns = []\n",
        "random_returns = []\n",
        "memory = []\n",
        "for episode in range(1000):\n",
        "  # Experience replay training\n",
        "  state = env.new_episode()\n",
        "  while True:\n",
        "    explore = np.random.rand() < 0.1\n",
        "    if explore:\n",
        "      action = np.random.choice(np.arange(3))\n",
        "    else:\n",
        "      action_values = np.array([QER[action].predict(state.reshape(1, -1)) \n",
        "                                for action in range(3)])\n",
        "      action = action_values.argmax()\n",
        "\n",
        "    next_state, reward, done = env.step(action)\n",
        "    memory.append((state, action, reward, next_state, float(done)))\n",
        "    memory = memory[-128:]\n",
        "    sample = [memory[index] for index in np.random.choice(len(memory),min(len(memory), 32), replace=False)]\n",
        "    states, actions, rewards, next_states, dones = map(np.stack, zip(*sample))\n",
        "    next_values = np.array([QER[action].predict(next_states) \n",
        "                      for action in range(3)])\n",
        "    for action in actions:\n",
        "      QER[action].partial_fit(states[actions == action], \n",
        "                            (rewards + (1-dones) * next_values.max(axis=0))[actions == action])\n",
        "    if not done:\n",
        "      state = next_state\n",
        "    else:\n",
        "      break\n",
        "   \n",
        "  \n",
        "  # Regular training\n",
        "  state = env.new_episode()\n",
        "  while True:\n",
        "    explore = np.random.rand() < 0.1\n",
        "    if explore:\n",
        "      action = np.random.choice(np.arange(3))\n",
        "    else:\n",
        "      action_values = np.array([Q[action].predict(state.reshape(1, -1)) \n",
        "                                for action in range(3)])\n",
        "      action = action_values.argmax()\n",
        "\n",
        "    next_state, reward, done = env.step(action)\n",
        "    if not done:\n",
        "      next_values = np.array([Q[action].predict(next_state.reshape(1, -1)) \n",
        "                        for action in range(3)])\n",
        "      Q[action].partial_fit(state.reshape(1, -1), [reward + next_values.max()])\n",
        "      state = next_state\n",
        "    else:\n",
        "      Q[action].partial_fit(state.reshape(1, -1), [reward])\n",
        "      break\n",
        "  \n",
        "  \n",
        "  \n",
        "  # Evaluation\n",
        "  RLER_return = np.mean([run_episode(\"RLER\") for _ in range(5)])\n",
        "  RLER_returns.append(RLER_return)\n",
        "  RL_return = np.mean([run_episode(\"RL\") for _ in range(5)])\n",
        "  RL_returns.append(RL_return)\n",
        "  random_return = np.mean([run_episode(\"random\") for _ in range(5)])\n",
        "  random_returns.append(random_return)\n",
        "  if episode % 200 == 0:\n",
        "    print(f\"Episode {episode}\"\n",
        "          f\", RL agent with ER return was {RLER_return:.2f}\"\n",
        "          f\", RL agent return was {RL_return:.2f}\"\n",
        "          f\", random agent return was {random_return:.2f}\")\n",
        "print(\"Done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0qr4Zj9bpn3",
        "colab_type": "text"
      },
      "source": [
        "And the training curves look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_gefcQ9780h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smoothed_RLER_returns = np.convolve(RLER_returns, np.ones((100,))/100, mode='valid')\n",
        "smoothed_RL_returns = np.convolve(RL_returns, np.ones((100,))/100, mode='valid')\n",
        "smoothed_random_returns = np.convolve(random_returns, np.ones((100,))/100, mode='valid')\n",
        "\n",
        "plt.plot(np.arange(len(smoothed_RLER_returns)), smoothed_RLER_returns, '-', \n",
        "         np.arange(len(smoothed_RL_returns)), smoothed_RL_returns, '-', \n",
        "         np.arange(len(smoothed_random_returns)), smoothed_random_returns, '-')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhVBh-Do5fdS",
        "colab_type": "text"
      },
      "source": [
        "So much more efficient!"
      ]
    }
  ]
}